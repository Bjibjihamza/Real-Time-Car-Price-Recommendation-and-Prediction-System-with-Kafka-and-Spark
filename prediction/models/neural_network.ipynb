{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "005fab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 09:36:56.407399: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-15 09:36:56.616802: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-05-15 09:36:56.616822: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-05-15 09:36:56.648974: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-15 09:36:57.605329: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-05-15 09:36:57.605431: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-05-15 09:36:57.605440: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfbf0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"ImprovedNeuralNetwork\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "df = spark.read.csv(\"data_preprocessed.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "366d908a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 14416\n",
      "Number of features: 36\n"
     ]
    }
   ],
   "source": [
    "# Convert to Pandas right away\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Print data information\n",
    "print(f\"Total samples: {pandas_df.shape[0]}\")\n",
    "print(f\"Number of features: {pandas_df.shape[1] - 1}\")\n",
    "\n",
    "# Separate features and target\n",
    "X = pandas_df.drop('price', axis=1)\n",
    "y = pandas_df['price']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a27f0be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features using RobustScaler\n",
    "feature_scaler = RobustScaler()\n",
    "X_scaled = feature_scaler.fit_transform(X)\n",
    "\n",
    "# Scale target variable\n",
    "target_scaler = RobustScaler()\n",
    "y_scaled = target_scaler.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Define k-fold cross validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c7b2c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store metrics across folds\n",
    "r2_scores = []\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "fold_predictions = []\n",
    "fold_actuals = []\n",
    "fold_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "865fccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_improved_model(input_dim):\n",
    "    model = Sequential([\n",
    "        # First layer - wider\n",
    "        Dense(64, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0005), input_shape=(input_dim,)),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Second layer\n",
    "        Dense(32, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0005)),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Third layer\n",
    "        Dense(16, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0005)),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Fourth layer - added an extra layer for more capacity\n",
    "        Dense(8, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0005)),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.1),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # IMPROVED: Compile with better optimizer settings\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer, loss='huber')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6544c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models across folds\n",
    "print(f\"Training {k_folds} models with cross-validation...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled)):\n",
    "    print(f\"\\nFold {fold+1}/{k_folds}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train, y_val = y_scaled[train_idx], y_scaled[val_idx]\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_improved_model(X_scaled.shape[1])\n",
    "    \n",
    "    # IMPROVED: Better callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=30,\n",
    "        restore_best_weights=True, \n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.2, \n",
    "        patience=10, \n",
    "        min_lr=0.000005, \n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        f'best_model_fold_{fold+1}.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # IMPROVED: Better training settings\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=500,  # More epochs but with early stopping\n",
    "        batch_size=32,  # Smaller batch size for better generalization\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping, reduce_lr, model_checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Load the best model from checkpoint\n",
    "    model = load_model(f'best_model_fold_{fold+1}.h5')\n",
    "    fold_models.append(model)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_pred_scaled = model.predict(X_val)\n",
    "    y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "    y_true = target_scaler.inverse_transform(y_val.reshape(-1, 1))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    r2_scores.append(r2)\n",
    "    rmse_scores.append(rmse)\n",
    "    mae_scores.append(mae)\n",
    "    \n",
    "    # Store predictions and actuals for ensemble\n",
    "    fold_predictions.append(y_pred)\n",
    "    fold_actuals.append(y_true)\n",
    "    \n",
    "    # Print fold results\n",
    "    print(f\"Fold {fold+1} - RMSE: {rmse:.2f}, MAE: {mae:.2f}, R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print average metrics\n",
    "print(\"\\nCross-validation results:\")\n",
    "print(f\"Average R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores):.2f} ± {np.std(rmse_scores):.2f}\")\n",
    "print(f\"Average MAE: {np.mean(mae_scores):.2f} ± {np.std(mae_scores):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec567a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED: Create weighted ensemble prediction\n",
    "print(\"\\nCreating weighted ensemble prediction...\")\n",
    "\n",
    "def ensemble_predict(models, X_data, weights=None):\n",
    "    \"\"\"Create weighted ensemble predictions from multiple models\"\"\"\n",
    "    predictions = np.array([model.predict(X_data).flatten() for model in models])\n",
    "    \n",
    "    if weights is None:\n",
    "        # Use equal weights if none provided\n",
    "        weights = np.ones(len(models)) / len(models)\n",
    "    \n",
    "    # Weighted average of predictions\n",
    "    weighted_predictions = np.sum(predictions.T * weights, axis=1)\n",
    "    return weighted_predictions.reshape(-1, 1)\n",
    "\n",
    "# Use inverse of validation RMSE as weights (better models get higher weights)\n",
    "inverse_rmse = [1/score for score in rmse_scores]\n",
    "weights = np.array(inverse_rmse) / sum(inverse_rmse)\n",
    "print(f\"Ensemble weights based on validation performance: {weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31eb686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine validation sets and make predictions for ensemble evaluation\n",
    "# This is for evaluation only - in practice you'd use a separate test set\n",
    "all_preds = []\n",
    "all_actuals = []\n",
    "\n",
    "for i, (_, val_idx) in enumerate(kf.split(X_scaled)):\n",
    "    X_val = X_scaled[val_idx]\n",
    "    y_val = y_scaled[val_idx]\n",
    "    \n",
    "    # Get ensemble prediction for this validation fold\n",
    "    y_pred_scaled = ensemble_predict([fold_models[j] for j in range(k_folds) if j != i], \n",
    "                                    X_val, \n",
    "                                    weights=[weights[j] for j in range(k_folds) if j != i])\n",
    "    \n",
    "    # Convert back to original scale\n",
    "    y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "    y_true = target_scaler.inverse_transform(y_val.reshape(-1, 1))\n",
    "    \n",
    "    all_preds.append(y_pred)\n",
    "    all_actuals.append(y_true)\n",
    "\n",
    "# Concatenate all validation predictions and actuals\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_actuals = np.concatenate(all_actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ensemble metrics\n",
    "ensemble_mse = mean_squared_error(all_actuals, all_preds)\n",
    "ensemble_rmse = np.sqrt(ensemble_mse)\n",
    "ensemble_mae = mean_absolute_error(all_actuals, all_preds)\n",
    "ensemble_r2 = r2_score(all_actuals, all_preds)\n",
    "\n",
    "print(\"\\nWeighted Ensemble model results:\")\n",
    "print(f\"Ensemble RMSE: {ensemble_rmse:.2f}\")\n",
    "print(f\"Ensemble MAE: {ensemble_mae:.2f}\")\n",
    "print(f\"Ensemble R²: {ensemble_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48034ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_model(input_dim):\n",
    "    model = Sequential([\n",
    "        # First layer\n",
    "        Dense(96, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0003), input_shape=(input_dim,)),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Second layer\n",
    "        Dense(48, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0003)),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Third layer\n",
    "        Dense(24, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0003)),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Fourth layer\n",
    "        Dense(12, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0003)),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.1),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # Compile with optimal settings\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer, loss='huber')\n",
    "    return model\n",
    "\n",
    "final_model = create_final_model(X_scaled.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e32d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=40,  # More patience for final model\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.2, \n",
    "    patience=15, \n",
    "    min_lr=0.000001\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'best_final_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3493585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with a fixed validation split\n",
    "val_split = 0.2\n",
    "n_val = int(len(X_scaled) * val_split)\n",
    "indices = np.random.permutation(len(X_scaled))\n",
    "X_train_final = X_scaled[indices[n_val:]]\n",
    "y_train_final = y_scaled[indices[n_val:]]\n",
    "X_val_final = X_scaled[indices[:n_val]]\n",
    "y_val_final = y_scaled[indices[:n_val]]\n",
    "\n",
    "# Train\n",
    "final_history = final_model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_final, y_val_final),\n",
    "    callbacks=[early_stopping, reduce_lr, model_checkpoint],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e3979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best final model\n",
    "final_model = load_model('best_final_model.h5')\n",
    "\n",
    "# Evaluate final model\n",
    "y_pred_scaled = final_model.predict(X_val_final)\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_true = target_scaler.inverse_transform(y_val_final.reshape(-1, 1))\n",
    "\n",
    "final_mse = mean_squared_error(y_true, y_pred)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_mae = mean_absolute_error(y_true, y_pred)\n",
    "final_r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(\"\\nFinal model validation results:\")\n",
    "print(f\"Final RMSE: {final_rmse:.2f}\")\n",
    "print(f\"Final MAE: {final_mae:.2f}\")\n",
    "print(f\"Final R²: {final_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec6237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model.save('improved_final_model.h5')\n",
    "print(\"Final model saved as 'improved_final_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8b34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
