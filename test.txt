% Setting up the document class
\documentclass[a4paper,11pt]{report}

% Including essential packages in a logical order
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{array}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{titlesec}
\usepackage{titling}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{fontawesome}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{caption}
\usepackage{float}
\usepackage{lmodern} % Adding a reliable font package
\usepackage{hyperref} % Loading hyperref last to avoid conflicts






% ---------------exp

\usepackage[skins,breakable]{tcolorbox} % Enable breakable boxes
\usepackage{listings}
\usepackage{xcolor}

% Define colors
\definecolor{cellbackground}{RGB}{248,248,255} % Adjusted to match your 'cellbackground'
\definecolor{cellborder}{RGB}{128,128,128}    % Gray border
\definecolor{incolor}{RGB}{0,0,205}           % Dark blue for prompts
\definecolor{outcolor}{RGB}{0,0,205}          % Dark blue for output prompts

% Define Python style for listings
\lstdefinestyle{pythonstyle}{
    language=Python,
    backgroundcolor=\color{cellbackground},
    keywordstyle=\color{incolor}\bfseries,
    stringstyle=\color{red},
    commentstyle=\color{gray},
    basicstyle=\ttfamily\small,
    frame=single,
    rulecolor=\color{cellborder},
    showstringspaces=false,
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1,
    numbersep=5pt,
    breaklines=true,
    breakatwhitespace=true,
}

% Command for Jupyter-like prompts
\newcommand{\prompt}[4]{%
    \makebox[0pt][r]{\makebox[#1][l]{\color{#2}#3}}\hspace{#4}%
}
\newlength{\boxspacing}
\setlength{\boxspacing}{2em} % Define boxspacing as a length

% Ensure compatibility with tcolorbox
\tcbset{
    enhanced,
    breakable,
    size=fbox,
    boxrule=1pt,
    pad at break*=1mm,
    colback=cellbackground,
    colframe=cellborder,
}




% ----------------end exp

% Setting PGFPlots compatibility
\pgfplotsset{compat=1.18}

% Configuring geometry
\geometry{left=2cm, right=2cm, top=2cm, bottom=2cm}

% Defining colors
\definecolor{maincolor}{RGB}{34, 45, 50}
\definecolor{sectioncolor}{RGB}{0, 102, 204}
\definecolor{subsectioncolor}{RGB}{46, 134, 193}
\definecolor{subsubsectioncolor}{rgb}{0.2, 0.6, 0.2} % Green for subsubsection
\definecolor{codebg}{HTML}{151b23}
\definecolor{codetext}{HTML}{D4D4D4}
\definecolor{codekey}{HTML}{569CD6}
\definecolor{codestring}{HTML}{CE9178}
\definecolor{kafka}{RGB}{35, 45, 63}
\definecolor{spark}{RGB}{226, 78, 27}
\definecolor{mongodb}{RGB}{77, 179, 61}
\definecolor{airflow}{RGB}{50, 168, 168}
\definecolor{cassandra}{RGB}{0, 128, 128}
\definecolor{ml}{RGB}{100, 149, 237}
\definecolor{web}{RGB}{124, 252, 0}
\definecolor{ywaarde}{RGB}{255, 245, 200} % Defining the previously undefined color



%color for sqlstyle 
\definecolor{codebg}{rgb}{0.95,0.95,0.95}    % light gray background
\definecolor{codegray}{rgb}{0.4,0.4,0.4}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{codeblue}{rgb}{0,0,0.9}
\definecolor{codered}{rgb}{0.6,0,0}

% Configuring headers and footers
\pagestyle{fancy}
\fancyhead[R]{\thepage}

% Loading TikZ libraries
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, shapes, arrows, fit, backgrounds}

% Defining TikZ styles using \tikzset (replacing deprecated \tikzstyle)
\tikzset{
  process/.style={rectangle, rounded corners, minimum height=3em, minimum width=6em, text centered, draw=black, fill=blue!20, text=black},
  database/.style={cylinder, shape border rotate=90, aspect=0.5, minimum height=3em, minimum width=4em, text centered, draw=black, fill=cassandra!20, text=black},
  arrow/.style={thick, -Stealth, >=stealth},
}

% Customizing section and subsection formatting
\titleformat{\section}{\color{sectioncolor}\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\color{subsectioncolor}\Large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\color{subsubsectioncolor}\normalfont\bfseries}{\thesubsubsection}{1em}{}

% Configuring listings for code blocks
\lstdefinestyle{sqlstyle}{
  backgroundcolor=\color{codebg},
  basicstyle=\small\ttfamily,
  language=SQL,
  keywordstyle=\color{codeblue}\bfseries,
  stringstyle=\color{codepurple},
  commentstyle=\color{codegray}\itshape,
  identifierstyle=\color{black},
  morekeywords={VARCHAR,INT,DATE,PRIMARY,KEY,FOREIGN,REFERENCES}, % you can extend this list
  breaklines=true,
  frame=single,
  showstringspaces=false
}







%==========Python styling ==================================
% Custom style ONLY for Python
% Define Jupyter Light-like colors
\definecolor{cellbg}{RGB}{248,248,255}         % Light notebook background
\definecolor{keywordcolor}{RGB}{0,0,205}       % Dark blue keywords
\definecolor{commentcolor}{RGB}{0,128,0}       % Green comments
\definecolor{stringcolor}{RGB}{163,21,21}      % Dark red strings
\definecolor{numbercolor}{gray}{0.5}


\lstdefinestyle{pythonstyle}{
  language=Python,
  backgroundcolor=\color{cellbg},
  basicstyle=\ttfamily\small,
  keywordstyle=\color{keywordcolor}\bfseries,
  commentstyle=\color{commentcolor}\itshape,
  stringstyle=\color{stringcolor},
  showstringspaces=false,
  breaklines=true,
  frame=single,
  rulecolor=\color{gray!20},
  numbers=left,
  numberstyle=\tiny\color{numbercolor},
  numbersep=8pt,
  xleftmargin=1em,
  framexleftmargin=1em,
  tabsize=4,
  captionpos=b
}









%==============================================================
% Suppressing default author and date display
\preauthor{}
\postauthor{}
\predate{}
\postdate{}

% Configuring code block style
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{codebg},
  basicstyle=\ttfamily\small\color{codetext},
  keywordstyle=\color{codekey},
  stringstyle=\color{codestring},
  commentstyle=\color{gray!60},
  frame=single,
  rulecolor=\color{codebg},
  breaklines=true,
  columns=fullflexible,
  xleftmargin=1cm,
  xrightmargin=1cm,
  showstringspaces=false,
  captionpos=b
}
\lstset{style=mystyle}

% Configuring TikZ styles for diagrams
\tikzset{
  database/.style={cylinder, draw, shape border rotate=90, aspect=0.3, minimum height=2cm, minimum width=1.5cm},
  process/.style={rectangle, draw, minimum width=2cm, minimum height=1cm, text centered},
  arrow/.style={thick, ->, >=stealth}
}

\begin{document}

\thispagestyle{empty}
\begin{center}
    \includegraphics[width=14cm]{images/logo.png}
\end{center}

\vspace{1cm}

\begin{center}
    \begin{tcolorbox}[colframe=sectioncolor, colback=white, arc=5mm, width=16cm]
        \vspace{0.4cm}
        \centering
        {\LARGE \textbf{Système de Recommandation et Prédiction des Prix }\\
        \vspace{0.4cm}
        {\Large \textbf{de Voitures au Maroc }}}\\
        \vspace{0.4cm}
    \end{tcolorbox}
\end{center}
\hspace{13cm} \textbf{Fili\`ere :} BDIA 1\vspace{.15cm}\\

\vspace{2cm}


% ---- Informations personnelles ----
\noindent\textbf{Présenté par :}
\vspace{0.5cm}

\begin{tabular}{p{6cm}p{9cm}}
Hamza Bjibji                & \textit{bjiji.hamza@etu.uae.ac.ma} \\
El Akramine Yassir Salim    & \textit{elakramine.yassirsalim@etu.uae.ac.ma} \\
Chaimae Ben Sbeh           & \textit{bensbeh.chaimae@etu.uae.ac.ma} \\
Imane Sghiouar          & \textit{sghiouar.imane@etu.uae.ac.ma} \\

\end{tabular}

\vspace{2cm}

\noindent\textbf{Encadré par :}\\
Prof. IMANE HACHCHANE



% ---- Bas de page ----
\vspace{3cm}
\begin{center}
    \textit{A.U 2025-2026}
\end{center}

\newpage






\begin{center}
    \LARGE\textbf{Résumé}
\end{center}


\Large Ce projet s'inscrit dans le domaine du Machine learning et a pour objectif la conception et l'implémentation d'un système de recommandation et de prédiction des prix de voitures au Maroc. Notre solution repose sur une architecture distribuée et évolutive qui collecte, traite et analyse en temps réel les données provenant des plateformes \textbf{avito.com} et \textbf{moteur.ma}.

Le système est construit autour d'un pipeline de données comprenant des étapes de web scraping, de traitement en streaming via Kafka, d'analyse avec Spark sur un cluster de quatre machines, de stockage dans MongoDB et enfin de visualisation à travers une interface web interactive. Cette approche permet de générer des recommandations personnalisées pour les utilisateurs ainsi que des prédictions de prix précises basées sur les tendances actuelles du marché automobile marocain.

Les résultats obtenus démontrent la pertinence de l'utilisation des technologies Big Data dans l'analyse du marché automobile et offrent aux consommateurs un outil fiable pour leurs décisions d'achat.

\thispagestyle{empty} % Pas d'en-tête 

\tableofcontents
\newpage


% ============== Chapitre 4 =====================
\chapter{Développement des Modèles de Recommandation et Prédiction}


\section{Introduction}

Ce chapitre présente les modèles d'analyse et les algorithmes développés pour le système de recommandation et de prédiction des prix de voitures au Maroc. Le système s'appuie sur des techniques d'apprentissage non supervisé pour les recommandations et des modèles d'apprentissage supervisé pour la prédiction des prix. De plus, un pipeline robuste de collecte et de traitement des données, basé sur le web scraping, Apache Kafka et Apache Spark, garantit la disponibilité de données fiables en temps réel. Ce chapitre détaille chaque composant, de la collecte des données à l'implémentation des algorithmes, en mettant en évidence leur rôle dans l'atteinte des objectifs du projet.



\section{Système de recommandation}

\subsection{Énoncé du problème}
La recommandation de voitures personnalisées est un défi clé dans le domaine automobile, où les utilisateurs recherchent des véhicules correspondant à leurs préférences explicites (marque, budget, caractéristiques) et à leurs interactions implicites (vues, favoris). L’objectif est de concevoir un système de recommandation hybride qui combine des approches basées sur le contenu (\textit{content-based filtering}), le filtrage collaboratif (\textit{collaborative filtering}), et un modèle hybride pour fournir des suggestions pertinentes et interprétables. \\

Ce problème est formulé comme une tâche de recommandation où les entrées incluent :
\begin{itemize}
    \item \textbf{Préférences utilisateur} : marque préférée, budget maximum, kilométrage souhaité, type de carburant, transmission (\texttt{user\_preferences}).
    \item \textbf{Interactions utilisateur} : vues (\texttt{car\_views\_by\_user}) et favoris (\texttt{favorite\_cars\_by\_user}) avec horodatages.
    \item \textbf{Attributs des voitures} : marque, prix (prédit via LightGBM), kilométrage, année, carburant, équipements (\texttt{cleaned\_cars}).
\end{itemize}

Les défis incluent la gestion de données hétérogènes (numériques, catégoriques, temporelles), la forte sparsity des interactions (e.g., 25 utilisateurs, 138 voitures, sparsity ~99.5\%), et la nécessité de fournir des raisons interprétables pour les recommandations (e.g., "Correspond à votre préférence pour Toyota diesel"). La complexité algorithmique est un facteur critique pour garantir une exécution rapide dans un contexte en temps réel.

\subsection{Prétraitement des données}
Le prétraitement transforme les données brutes stockées dans Apache Cassandra (\texttt{cars\_keyspace}) en un format adapté aux algorithmes de recommandation, tout en réutilisant les transformations du pipeline de prédiction des prix.

\subsubsection{Chargement et préparation initiale}
Les données sont extraites des tables suivantes :
\begin{itemize}
    \item \textbf{\texttt{cleaned\_cars}} : Attributs des voitures (e.g., \texttt{car\_id}, \texttt{brand}, \texttt{price}, \texttt{mileage}).
    \item \textbf{\texttt{user\_preferences}} : Préférences utilisateur (e.g., \texttt{preferred\_brands}, \texttt{budget\_max}).
    \item \textbf{\texttt{car\_views\_by\_user}}, \textbf{\texttt{favorite\_cars\_by\_user}} : Interactions avec horodatages.
\end{itemize}

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Tableau} & \textbf{Colonnes principales} & \textbf{Type} & \textbf{Exemple} \\
\hline
\texttt{cleaned\_cars} & car\_id, brand, price, mileage, year & UUID, texte, double & \{UUID, "Toyota", 150000, 80000, 2018\} \\
\texttt{user\_preferences} & user\_id, preferred\_brands, budget\_max & UUID, liste, double & \{UUID, ["Toyota", "Honda"], 200000\} \\
\texttt{car\_views\_by\_user} & user\_id, car\_id, view\_timestamp & UUID, UUID, timestamp & \{UUID, UUID, "2025-02-11 10:00"\} \\
\texttt{favorite\_cars\_by\_user} & user\_id, car\_id, favorite\_timestamp & UUID, UUID, timestamp & \{UUID, UUID, "2025-02-10 15:00"\} \\
\hline
\end{tabular}
}
\caption{Métadonnées des tables de recommandation}
\label{tab:recommendation_data}
\end{table}

Le chargement utilise le driver Python Cassandra avec une politique \texttt{DCAwareRoundRobinPolicy} :
\begin{lstlisting}[style=pythonstyle]
def fetch_data(session, table, columns, condition=None):
    query = f"SELECT {', '.join(columns)} FROM cars_keyspace.{table}"
    if condition:
        query += f" WHERE {condition}"
    rows = session.execute(query)
    return pd.DataFrame([row._asdict() for row in rows])
\end{lstlisting}

\subsubsection{Traitement des valeurs manquantes}
Les valeurs manquantes sont imputées comme suit, en cohérence avec le pipeline de prédiction :
\begin{itemize}
    \item \textbf{Numériques} : Médiane (e.g., \texttt{price}, \texttt{mileage}).
    \item \textbf{Catégoriques} : Mode (e.g., \texttt{fuel\_type = "Diesel"}).
    \item \textbf{Préférences absentes} : Valeurs par défaut (e.g., budget max = médiane des prix).
\end{itemize}

\begin{lstlisting}[style=pythonstyle]
def handle_missing_values(df, numeric_cols, categorical_cols):
    for col in numeric_cols:
        df[col].fillna(df[col].median(), inplace=True)
    for col in categorical_cols:
        df[col].fillna(df[col].mode()[0], inplace=True)
    return df
\end{lstlisting}

\subsubsection{Ingénierie des caractéristiques}
Les transformations incluent :
\begin{itemize}
    \item \textbf{Standardisation} : Textes en minuscules (e.g., \texttt{brand}).
    \item \textbf{Encodage one-hot} : Variables catégoriques (\texttt{brand}, \texttt{fuel\_type}).
    \item \textbf{Normalisation} : \texttt{MinMaxScaler} pour les numériques (\texttt{price}, \texttt{mileage}).
    \item \textbf{Poids de récence} : Pondération des interactions selon leur ancienneté (décroissance sur 30 jours).
\end{itemize}

\begin{lstlisting}[style=pythonstyle]
def recency_weight(timestamp, current_time):
    days_old = (current_time - timestamp).total_seconds() / (24 * 3600)
    return max(0.5, 1.0 - (days_old / 30.0))
\end{lstlisting}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Interaction} & \textbf{Timestamp} & \textbf{Days Old} & \textbf{Recency Weight} \\
\hline
Vue & 2025-02-11 10:00 & 3 & 0.90 \\
Favori & 2025-01-15 15:00 & 30 & 0.50 \\
Vue & 2024-12-01 08:00 & 75 & 0.50 \\
\hline
\end{tabular}
\caption{Pondération par récence des interactions}
\label{tab:recency_weight}
\end{table}

\subsection{Présentation des métriques d’évaluation}
Les métriques évaluent la pertinence, la diversité et l’efficacité des recommandations :
\begin{itemize}
    \item \textbf{Précision@K} : Proportion des K recommandations correspondant aux favoris :
    \[
    \text{Précision@K} = \frac{\text{Nombre de recommandations pertinentes dans les K premières}}{\text{K}}
    \]
    (K=5 pour les tests).
    \item \textbf{Couverture} : Proportion des voitures recommandées au moins une fois.
    \item \textbf{Temps de calcul} : Temps d’exécution, crucial pour le backend Node.js.
\end{itemize}

\begin{lstlisting}[style=pythonstyle]
def evaluate_precision(recommendations, favorites, k=5):
    user_recs = recommendations.groupby('user_id').head(k)
    relevant = user_recs.merge(favorites, on=['user_id', 'car_id'], how='inner')
    return len(relevant) / (len(user_recs) if len(user_recs) > 0 else 1)
\end{lstlisting}

\subsection{Algorithme 1 : Filtrage collaboratif basé sur les utilisateurs}
\subsubsection{Architecture de l’algorithme}
Le filtrage collaboratif basé sur les utilisateurs (\textit{user-based CF}) construit une matrice utilisateur-objet, calcule la similarité entre utilisateurs, et recommande des voitures appréciées par des utilisateurs similaires.

\subsubsection{Description de l’algorithme}
\begin{enumerate}
    \item Créer une matrice utilisateur-objet (\texttt{user\_item\_matrix}) : lignes = utilisateurs (\(M=25\)), colonnes = voitures (\(N=138\)), valeurs = scores d’interaction (vue=1.0, favori=2.0, pondéré par récence).
    \item Calculer la similarité cosinus entre utilisateurs :
    \[
    \text{sim}(u_i, u_j) = \frac{\vec{u_i} \cdot \vec{u_j}}{\|\vec{u_i}\| \|\vec{u_j}\|}
    \]
    \item Sélectionner les 20 utilisateurs les plus similaires (\texttt{top\_k=20}).
    \item Agréger les voitures non vues/non favorites, pondérées par les scores de similarité.
    \item Retourner les 3 voitures avec les scores les plus élevés.
\end{enumerate}

\begin{lstlisting}[style=pythonstyle]
user_item_matrix = pd.pivot_table(interactions, values='score', index='user_id', columns='car_id')
user_similarity_matrix = cosine_similarity(user_item_matrix.fillna(0))
similar_users = user_similarity_df.loc[user_id].sort_values(ascending=False)[1:21]
candidate_scores = {}
for similar_user_id, sim_score in similar_users.items():
    if sim_score > 0.1:
        similar_user_cars = set(interactions[interactions['user_id'] == similar_user_id]['car_id'])
        for car_id in similar_user_cars - excluded_cars:
            candidate_scores[car_id] = candidate_scores.get(car_id, 0) + sim_score
\end{lstlisting}

\subsubsection{Complexité}
\begin{itemize}
    \item \textbf{Temporelle} :
        \begin{itemize}
            \item Construction de la matrice : \(O(I)\), où \(I\) est le nombre d’interactions (\(I \ll M \cdot N\) en raison de la sparsity).
            \item Similarité cosinus : \(O(M^2 \cdot N)\) pour \(M\) utilisateurs et \(N\) voitures.
            \item Sélection des voisins : \(O(M \log M)\) par utilisateur pour trier.
            \item Agrégation des scores : \(O(k \cdot N)\), où \(k=20\) est le nombre de voisins.
            \item Total par utilisateur : \(O(M^2 \cdot N + M \log M + k \cdot N)\), dominé par \(O(M^2 \cdot N)\).
        \end{itemize}
    \item \textbf{Spatiale} :
        \begin{itemize}
            \item Matrice utilisateur-objet : \(O(M \cdot N)\), bien que sparse.
            \item Matrice de similarité : \(O(M^2)\).
            \item Total : \(O(M \cdot N + M^2)\).
        \end{itemize}
\end{itemize}
Pour \(M=25\), \(N=138\), la complexité temporelle est gérable, mais elle croît rapidement avec \(M\).

\subsubsection{Justification du choix}
\begin{itemize}
    \item Exploite les comportements implicites (vues, favoris) pour capturer les préférences collectives.
    \item Efficace pour les utilisateurs avec un historique riche, malgré la sparsity.
\end{itemize}

\subsubsection{Forces et faiblesses}
\textbf{Forces} : Personnalisation basée sur les comportements ; robuste avec des interactions denses. \\
\textbf{Faiblesses} : Problème de cold-start pour les nouveaux utilisateurs ; coûteux pour un grand \(M\).

\subsubsection{Évaluation}
Précision@5 d’environ 0.15, limitée par la sparsity (\(\sim 99.5\%\)).

\subsection{Algorithme 2 : Filtrage collaboratif basé sur les objets}
\subsubsection{Architecture de l’algorithme}
Le filtrage collaboratif basé sur les objets (\textit{item-based CF}) recommande des voitures similaires à celles déjà vues ou favorites, en fonction des interactions globales.

\subsubsection{Description de l’algorithme}
\begin{enumerate}
    \item Utiliser la matrice utilisateur-objet pour calculer la similarité cosinus entre voitures.
    \item Pour chaque voiture dans l’historique de l’utilisateur, identifier les voitures similaires.
    \item Agréger les scores de similarité pour les voitures non vues/non favorites.
    \item Retourner les 3 meilleures voitures.
\end{enumerate}

\begin{lstlisting}[style=pythonstyle]
car_similarity_matrix = cosine_similarity(user_item_matrix.T.fillna(0))
for car_id in user_interactions:
    similar_cars = car_similarity_df.loc[car_id].dropna()
    for similar_car_id, sim_score in similar_cars.items():
        if similar_car_id not in user_interactions and sim_score > 0.1:
            candidate_scores[similar_car_id] = candidate_scores.get(similar_car_id, 0) + sim_score
\end{lstlisting}

\subsubsection{Complexité}
\begin{itemize}
    \item \textbf{Temporelle} :
        \begin{itemize}
            \item Similarité cosinus : \(O(N^2 \cdot M)\) pour \(N\) voitures et \(M\) utilisateurs.
            \item Agrégation par utilisateur : \(O(I_u \cdot N)\), où \(I_u\) est le nombre d’interactions de l’utilisateur (\(I_u \ll N\)).
            \item Total par utilisateur : \(O(N^2 \cdot M + I_u \cdot N)\), dominé par \(O(N^2 \cdot M)\).
        \end{itemize}
    \item \textbf{Spatiale} :
        \begin{itemize}
            \item Matrice utilisateur-objet : \(O(M \cdot N)\).
            \item Matrice de similarité : \(O(N^2)\).
            \item Total : \(O(M \cdot N + N^2)\).
        \end{itemize}
\end{itemize}
Pour \(N=138\), la complexité est plus favorable que user-based CF pour un grand \(M\).

\subsubsection{Justification du choix}
\begin{itemize}
    \item Efficace pour les utilisateurs avec peu d’interactions.
    \item Exploite les similarités entre voitures, moins sensible au cold-start utilisateur.
\end{itemize}

\subsubsection{Forces et faiblesses}
\textbf{Forces} : Rapide pour les recommandations ; bonne couverture. \\
\textbf{Faiblesses} : Moins personnalisé ; dépend de la densité des interactions.

\subsubsection{Évaluation}
Précision@5 de 0.15, mais couverture supérieure (\(\sim 75\%\)) grâce à la diversité des recommandations.

\subsection{Algorithme 3 : Filtrage basé sur le contenu}
\subsubsection{Architecture de l’algorithme}
Le filtrage basé sur le contenu (\textit{content-based filtering}) construit des vecteurs de caractéristiques pour les utilisateurs et les voitures, puis recommande en fonction de la similarité cosinus.

\subsubsection{Description de l’algorithme}
\begin{enumerate}
    \item Construire des vecteurs pour :
        \begin{itemize}
            \item Utilisateurs : Préférences (\texttt{preferred\_brands}, \texttt{budget\_max}, etc.), encodées one-hot et normalisées.
            \item Voitures : Attributs (\texttt{brand}, \texttt{price}, etc.), encodés et normalisés.
        \end{itemize}
    \item Calculer la similarité cosinus :
    \[
    \text{sim}(u, c) = \frac{\vec{u} \cdot \vec{c}}{\|\vec{u}\| \|\vec{c}\|}
    \]
    \item Sélectionner les 3 voitures avec les scores les plus élevés.
    \item Générer des raisons (e.g., "Marque Toyota, diesel").
\end{enumerate}

\begin{lstlisting}[style=pythonstyle]
user_features_df = pd.concat([user_cat_df, user_num_df], axis=1)
car_features_df = pd.concat([car_cat_df, car_num_df], axis=1)
similarity_matrix = cosine_similarity(user_features_df, car_features_df)
top_cars = similarity_df.loc[user_id].nlargest(3)
reasons = [f"Matches preferences: {', '.join(matched_attrs)}" for matched_attrs in matched_attrs_list]
\end{lstlisting}

\subsubsection{Complexité}
\begin{itemize}
    \item \textbf{Temporelle} :
        \begin{itemize}
            \item Construction des vecteurs : \(O((M + N) \cdot F)\), où \(F\) est le nombre de caractéristiques (\(F \approx 50\) après encodage).
            \item Similarité cosinus : \(O(M \cdot N \cdot F)\).
            \item Sélection des top 3 : \(O(N \log N)\).
            \item Total par utilisateur : \(O(N \cdot F + N \log N)\).
        \end{itemize}
    \item \textbf{Spatiale} :
        \begin{itemize}
            \item Vecteurs utilisateur et voiture : \(O((M + N) \cdot F)\).
            \item Matrice de similarité : \(O(M \cdot N)\).
            \item Total : \(O((M + N) \cdot F + M \cdot N)\).
        \end{itemize}
\end{itemize}
La complexité est linéaire en \(N\), rendant l’algorithme rapide pour \(N=138\).

\subsubsection{Justification du choix}
\begin{itemize}
    \item Idéal pour les utilisateurs avec des préférences explicites.
    \item Intègre les prédictions de prix LightGBM pour respecter le budget.
    \item Fournit des recommandations interprétables.
\end{itemize}

\subsubsection{Forces et faiblesses}
\textbf{Forces} : Résout le cold-start utilisateur ; rapide et interprétable. \\
\textbf{Faiblesses} : Dépend des préférences ; moins efficace sans données explicites.

\subsubsection{Évaluation}
Précision@5 de 0.18, grâce à l’alignement avec les préférences.

\subsection{Algorithme 4 : Recommandation hybride}
\subsubsection{Architecture de l’algorithme}
Le modèle hybride combine le filtrage collaboratif (via SVD) et le filtrage basé sur le contenu, en pondérant leurs scores.

\subsubsection{Description de l’algorithme}
\begin{enumerate}
    \item Décomposer la matrice utilisateur-objet via SVD :
    \[
    R \approx U \Sigma V^T
    \]
    où \(k=20\) est le nombre de facteurs latents.
    \item Prédire les scores d’interaction : \(R_{\text{pred}} = U \Sigma V^T\).
    \item Calculer les scores de similarité basés sur le contenu.
    \item Combiner les scores :
    \[
    \text{Score hybride} = \alpha \cdot \text{Score SVD} + (1 - \alpha) \cdot \text{Score contenu}
    \]
    (\(\alpha=0.5\)).
    \item Sélectionner les 3 meilleures voitures avec raisons.
\end{enumerate}

\begin{lstlisting}[style=pythonstyle]
U, sigma, Vt = svds(user_item_matrix.values, k=min(20, min_dim-1))
R_pred = np.dot(np.dot(U, np.diag(sigma)), Vt)
collab_scores = MinMaxScaler().fit_transform(R_pred)
hybrid_scores = alpha * collab_scores + (1 - alpha) * content_scores
\end{lstlisting}

\subsubsection{Complexité}
\begin{itemize}
    \item \textbf{Temporelle} :
        \begin{itemize}
            \item SVD : \(O(\min(M^2 \cdot N, M \cdot N^2))\) pour une matrice sparse.
            \item Prédiction SVD : \(O(M \cdot N \cdot k)\).
            \item Similarité contenu : \(O(M \cdot N \cdot F)\).
            \item Combinaison : \(O(M \cdot N)\).
            \item Total : \(O(\min(M^2 \cdot N, M \cdot N^2) + M \cdot N \cdot (k + F))\).
        \end{itemize}
    \item \textbf{Spatiale} :
        \begin{itemize}
            \item Matrice utilisateur-objet : \(O(M \cdot N)\).
            \item Matrices SVD : \(O((M + N) \cdot k)\).
            \item Vecteurs contenu : \(O((M + N) \cdot F)\).
            \item Total : \(O(M \cdot N + (M + N) \cdot (k + F))\).
        \end{itemize}
\end{itemize}
La complexité est plus élevée, mais \(k=20\) et \(F \approx 50\) limitent l’impact.

\subsubsection{Justification du choix}
\begin{itemize}
    \item Combine les forces du collaboratif et du contenu.
    \item Atténue le cold-start et la sparsity via SVD.
\end{itemize}

\subsubsection{Forces et faiblesses}
\textbf{Forces} : Robuste et équilibré ; meilleure précision. \\
\textbf{Faiblesses} : Complexité accrue ; dépend du réglage d’\(\alpha\).

\subsubsection{Évaluation}
Précision@5 de 0.20, meilleure grâce à la complémentarité.

\subsection{Stratégie de génération et stockage}
Les recommandations sont générées pour un utilisateur via :
\begin{enumerate}
    \item Suppression des anciennes recommandations dans \texttt{user\_recommendations}.
    \item Exécution des 4 algorithmes (12 recommandations max).
    \item Stockage dans Cassandra avec UUID, rang, score, raison.
\end{enumerate}

\begin{lstlisting}[style=pythonstyle]
insert_query = """
    INSERT INTO user_recommendations (user_id, car_id, created_at, rank, recommendation_reason, similarity_score)
    VALUES (%s, %s, %s, %s, %s, %s)
"""
session.execute(insert_query, (uuid.UUID(user_id), uuid.UUID(car_id), created_at, rank, reason, score))
\end{lstlisting}

\subsection{Évaluation comparative des algorithmes}
\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Algorithme} & \textbf{Précision@5} & \textbf{Couverture (\%)} & \textbf{Temps (s)} & \textbf{Complexité temporelle} & \textbf{Complexité spatiale} \\
\hline
User-Based CF & 0.15 & 60 & 0.8 & \(O(M^2 \cdot N)\) & \(O(M \cdot N + M^2)\) \\
Item-Based CF & 0.15 & 75 & 0.6 & \(O(N^2 \cdot M)\) & \(O(M \cdot N + N^2)\) \\
Content-Based & 0.18 & 50 & 0.4 & \(O(N \cdot F)\) & \(O((M + N) \cdot F)\) \\
Hybride & 0.20 & 65 & 1.2 & \(O(\min(M^2 \cdot N, M \cdot N^2))\) & \(O(M \cdot N + (M + N) \cdot k)\) \\
\hline
\end{tabular}
\caption{Comparaison des algorithmes de recommandation}
\label{tab:recommendation_performance}
\end{table}

Le modèle hybride offre la meilleure précision@5 (0.20), mais est le plus coûteux. Content-based est le plus rapide et adapté au temps réel. Item-based CF maximise la couverture. Pour \(M=25\), \(N=138\), tous sont viables, mais content-based et item-based CF sont préférables pour un grand \(N\).

\section{Système de prédiction des prix}

\subsection{Énoncé du problème}
La prédiction du prix des voitures représente un défi majeur dans le secteur automobile, où de multiples facteurs influencent simultanément la valeur marchande d'un véhicule. Notre objectif est de développer un modèle capable d'estimer avec précision le prix d'une voiture à partir de ses caractéristiques techniques et commerciales. \\

Ce problème se formule comme une tâche de régression où la variable cible (le prix) est continue. Les variables explicatives comprennent divers attributs des véhicules tels que la marque, le modèle, l'année de fabrication, le kilométrage, la puissance, le type de carburant, la transmission, et d'autres spécifications techniques. La complexité du problème réside dans la nature hétérogène de ces caractéristiques (variables numériques et catégorielles) ainsi que dans les relations non linéaires potentielles entre ces attributs et le prix final.

\subsection{Prétraitement des données pour ML}

Le prétraitement des données constitue une étape cruciale dans notre système de prédiction des prix de voitures. Cette phase transforme les données brutes en un format exploitable par les modèles d'apprentissage automatique, tout en améliorant la qualité et la pertinence des informations utilisées. Notre processus de prétraitement se décompose en plusieurs étapes méthodiques.



\subsubsection{ Chargement et préparation initiale}

Nous avons commencé par charger les données brutes depuis un fichier CSV en utilisant PySpark, une bibliothèque adaptée au traitement de grands volumes de données. La première étape a consisté à éliminer les colonnes non pertinentes pour notre analyse prédictive, notamment : \\


\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Creator} & \textbf{Source} & \textbf{Image\_Folder} & \textbf{ID} & \textbf{Title} \\
\hline
MOHAMED & moteur & 417803\_MERCEDES-BENZ\_250 & 9b006bc1-dc96-4dc0-bdd2-0c31a28d555a & MERCEDES-BENZ 250 \\
Timlalin Dome & avito & 3449\_Mitsubishi\_Pajero & e851563f-cac5-4703-9f50-26d10d59b89e & Mitsubishi Pajero \\
Louvre AutoShopBadge Icon & avito & 3360\_Fiat\_Doblo\_Modèle & 43e6114c-7df9-4ebf-a611-4153e866324f & Fiat Doblo Modèle \\
imad elmajdoub & avito & 2694\_لاند\_روڨر\_ديسكوفري\_ديزل\_أوتوماتيك\_2017\_في\_أكادير & dd4a01b6-0cae-4e28-8c0f-28ee6844a823 & لاند روڨر ديسكوفري ديزل أوتوماتيك 2017 في أكادير \\
ayoub & avito & 700\_dacia\_duster & 8c94b460-b7a4-47ba-b84c-a2e146d0e5ff & dacia duster \\
\hline
\end{tabular}
}
\caption{Vehicle dataset metadata}
\label{tab:vehicle_data}
\end{table}


\begin{itemize}
    \item \textbf{creator :} l'identifiant du créateur de l'annonce
    \item \textbf{source :} la source de l'annonce
    \item \textbf{image\_folder :} le dossier contenant les images du véhicule
    \item \textbf{id :} l'identifiant unique de l'annonce
    \item \textbf{title :} le titre de l'annonce
\end{itemize}

Ces informations, bien que potentiellement utiles dans d'autres contextes, n'apportent pas de valeur ajoutée pour la prédiction du prix.


\subsubsection{Traitement des valeurs manquantes}

Les données présentaient plusieurs valeurs manquantes qui pouvaient compromettre la qualité de nos modèles. Nous avons adopté une approche rigoureuse en supprimant les observations contenant des valeurs manquantes . \\


\begin{lstlisting}[style=pythonstyle]
def handle_missing(df) :
    # Drop all the missing values :
    return df.dropna()
\end{lstlisting} 


\vspace{0.9mm}
Cette méthode, bien que diminuant légèrement la taille de notre jeu de données, garantit une meilleure fiabilité des prédictions en évitant les biais liés à l'imputation artificielle de valeurs.


\subsubsection{Ingénierie des caractéristiques}
Cette étape a permis d'enrichir notre jeu de données en créant de nouvelles variables pertinentes : \\

\textbf{Extraction temporelle} \\

La colonne \texttt{publication\_date} contient la date de mise en ligne de l'annonce. Cette information temporelle est cruciale pour analyser les dynamiques de publication et leur influence éventuelle sur les prix. \\

\begin{table}[H]
\centering
\begin{tabular}{|l|}
\hline
\textbf{publication\_date} \\
\hline
11/02/2025 00:00 \\
14/09/2024 00:00 \\
11/02/2025 00:00 \\
14/10/2024 00:00 \\
11/02/2025 00:00 \\
\hline
\end{tabular}
\caption{Exemples de valeurs de la colonne \texttt{publication\_date}}
\label{tab:publication_date_examples}
\end{table}


Nous avons décomposé la colonne publication\_date en plusieurs dimensions temporelles : 


\begin{itemize}
  \item Année de publication (\texttt{publication\_Year})
  \item Mois de publication (\texttt{publication\_Month})
  \item Jour du mois (\texttt{publication\_Day})
  \item Jour de la semaine (\texttt{publication\_Weekday})
  \item Indicateur de weekend (\texttt{Is\_Weekend})
  \item Nombre de jours depuis la publication (\texttt{Days\_since\_posted})
\end{itemize} 



\vspace{0.9mm}
\begin{lstlisting}[style=pythonstyle]
def extract_publication_date_features(df):
    """
    Extracts date-related features from the 'publication_date' column in a PySpark DataFrame.
    Returns the transformed DataFrame with new columns and drops the original 'publication_date' column.
    """

    # Convert publication_date to timestamp :
    df = df.withColumn("publication_date", F.to_timestamp("publication_date", "dd/MM/yyyy HH:mm"))

    # Create date-based features :
    df = df.withColumn("publication_Year", F.year("publication_date")) \
           .withColumn("publication_Month", F.month("publication_date")) \
           .withColumn("publication_Day", F.dayofmonth("publication_date")) \
           .withColumn("publication_Weekday", F.dayofweek("publication_date")) \
           .withColumn("Is_Weekend", (F.dayofweek("publication_date") >= 6).cast(IntegerType())) \
           .withColumn("Days_since_posted", F.datediff(F.current_date(), "publication_date"))

    # Drop original column :
    df = df.drop("publication_date")

    
    return df
\end{lstlisting} 


\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|l|l|l|l|l|}
\hline
publication\_Year & publication\_Month & publication\_Day & publication\_Weekday & Is\_Weekend & Days\_since\_posted \\
\hline
2025.0            & 2.0                & 11.0             & 3                    & 0           & 78.0                \\
2024.0            & 9.0                & 14.0             & 7                    & 1           & 228.0               \\
2025.0            & 2.0                & 11.0             & 3                    & 0           & 78.0                \\
2024.0            & 10.0               & 14.0             & 2                    & 0           & 198.0               \\
2025.0            & 2.0                & 11.0             & 3                    & 0           & 78.0               
\hline
\end{tabular}
}
\caption{Tableau montrant les dates de publication, les jours de la semaine et le nombre de jours écoulés depuis la publication.}
\end{table}


Ces nouvelles variables permettent de capturer d'éventuels effets saisonniers ou temporels sur les prix des véhicules. \\



\textbf{Traitement des équipements} \\

Les équipements des véhicules étaient initialement stockés dans une colonne unique sous forme de texte. \\


\begin{table}[ht]
\centering
\small
\begin{tabular}{|l|}
\hline
equipment \\ \hline
Airbags, Caméra de recul, Climatisation, ESP, Jantes aluminium, Ordinateur de bord, Radar de recul \\ \hline
ABS, Airbags, CD/MP3/Bluetooth, Caméra de recul, Climatisation, ESP, Jantes aluminium, Limiteur de vitesse \\ \hline
ABS, Airbags, CD/MP3/Bluetooth, Caméra de recul, Climatisation, ESP, Jantes aluminium, Limiteur de vitesse \\ \hline
ABS, Airbags, CD/MP3/Bluetooth, Climatisation, ESP, Jantes aluminium, Limiteur de vitesse, Ordinateur de bord \\ \hline
ABS, CD/MP3/Bluetooth, Caméra de recul, Climatisation, ESP, Jantes aluminium, Limiteur de vitesse \\ \hline
\end{tabular}
\caption{Liste des équipements disponibles sur différents modèles de véhicules.}
\end{table}


Nous avons transformé cette information en créant des variables binaires distinctes pour chaque type d'équipement : \\


\begin{itemize}
    \item Jantes aluminium (\textit{Alloy wheels})
    \item Airbags (\textit{Airbags})
    \item Climatisation (\textit{Air conditioning})
    \item Système de navigation (\textit{Navigation system})
    \item Toit ouvrant (\textit{Sunroof})
    \item Sièges en cuir (\textit{Leather seats})
    \item Radar de recul (\textit{Parking sensors})
    \item Caméra de recul (\textit{Rear camera})
    \item Vitres électriques (\textit{Electric windows})
    \item ABS (\textit{ABS})
    \item ESP (\textit{ESP})
    \item Régulateur de vitesse (\textit{Cruise control})
    \item Limiteur de vitesse (\textit{Speed limiter})
    \item CD/MP3/Bluetooth (\textit{CD/MP3/Bluetooth})
    \item Ordinateur de bord (\textit{On-board computer})
    \item Verrouillage centralisé (\textit{Central locking})
\end{itemize}


\vspace{0.9mm}
\begin{lstlisting}[style=pythonstyle]
def split_equipment(df, equipment_col="equipment"):
    """
    Splits the equipment column into multiple binary columns (True/False) based on known equipment types.
    """
    equipment_types = [
        "Jantes aluminium", "Airbags", "Climatisation", "Système de navigation",
        "Toit ouvrant", "Sièges en cuir", "Radar de recul", "Caméra de recul",
        "Vitres électriques", "ABS", "ESP", "Régulateur de vitesse", 
        "Limiteur de vitesse", "CD/MP3/Bluetooth", "Ordinateur de bord", "Verrouillage centralisé"
    ]
    
    for eq in equipment_types:
        new_col = eq.lower().replace(" ", "_").replace("/", "_").replace("-", "_") \
                    .replace("é", "e").replace("è", "e").replace("à", "a") \
                    .replace("ù", "u").replace("ç", "c")  # normalize French characters

        df = df.withColumn(
            new_col,
            when(lower(col(equipment_col)).contains(eq.lower()), lit(True)).otherwise(lit(False))
        )

        # Traduire les columns en français :
        column_mapping = {
        "jantes_aluminium": "Alloy_wheels",
        "airbags": "Airbags",
        "climatisation": "Air_conditioning",
        "navigation_system": "Navigation_system",
        "toit_ouvrant": "Sunroof",
        "sièges_cuir": "Leather_seats",
        "radar_de_recul": "Parking_sensors",
        "caméra_de_recul": "Rear_camera",
        "vitres_électriques": "Electric_windows",
        "abs": "ABS",
        "esp": "ESP",
        "régulateur_de_vitesse": "Cruise_control",
        "limiteur_de_vitesse": "Speed_limiter",
        "cd_mp3_bluetooth": "CD/MP3/Bluetooth",
        "ordinateur_de_bord": "On_board_computer",
        "verrouillage_centralisé": "Central_locking"
        }

        for old_name, new_name in column_mapping.items():
            df = df.withColumnRenamed(old_name, new_name)

    return df
\end{lstlisting} 



\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
Alloy\_wheels & Airbags & Air\_conditioning & Navigation\_system & Sunroof & Leather\_seats & Parking\_sensors & Rear\_camera \\ \hline
True          & True    & True              & False              & False   & False          & True             & False        \\ \hline
True          & True    & True              & False              & True    & False          & True             & False        \\ \hline
True          & True    & True              & False              & False   & False          & True             & False        \\ \hline
True          & True    & True              & False              & True    & False          & True             & False        \\ \hline
True          & False   & True              & False              & False   & False          & True             & False        \\ \hline
\end{tabular}%
}
\caption{Équipements Partie 1}
\end{table}


\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
Electric\_windows & ABS   & ESP  & Cruise\_control & Speed\_limiter & CD/MP3/Bluetooth & On\_board\_computer & Central\_locking \\ \hline
False             & False & True & False           & False          & False            & True                & False            \\ \hline
False             & True  & True & False           & True           & True             & True                & False            \\ \hline
False             & True  & True & False           & True           & True             & True                & False            \\ \hline
False             & True  & True & False           & True           & True             & True                & False            \\ \hline
\end{tabular}%
}
\caption{Équipements Partie 2}
\end{table}

Cette transformation permet aux modèles d'apprentissage de percevoir l'impact individuel de chaque équipement sur le prix final. \\






\textbf{Encodage des variables} \\

\textbf{1.Variables booléennes :} \\

Les variables binaires créées lors de l'étape précédente ont été converties en valeurs numériques (0 et 1). \\

\begin{lstlisting}[style=pythonstyle]
 bool_cols = ["Alloy_wheels", "Airbags", "Air_conditioning", "Navigation_system", "Sunroof", 
                 "Leather_seats", "Parking_sensors", "Rear_camera", "Electric_windows", "ABS",
                 "ESP", "Cruise_control", "Speed_limiter", "CD/MP3/Bluetooth", "On_board_computer", 
                 "Central_locking" ]

    for col_name in bool_cols:
        dt = dt.withColumn(col_name, col(col_name).cast(IntegerType()))
\end{lstlisting} 

\vspace{1mm}
\textbf{2.Variables numériques :} \\
Les colonnes numériques ont été explicitement converties au format double pour garantir une précision optimale : 


\begin{itemize}
    \item \textbf{door\_count}: Nombre de portes
    \item \textbf{fiscal\_power}: Puissance fiscale
    \item \textbf{mileage}: Kilométrage
    \item \textbf{price}: Prix (notre variable cible)
    \item \textbf{year}: Année du véhicule
    \item Et les variables temporelles créées précédemment
\end{itemize}

\begin{lstlisting}[style=pythonstyle]
# Cast numeric columns to correct type
    numeric_cols = ["door_count", "fiscal_power", "mileage", "price", "year", 
                       "publication_Year", "publication_Month", "publication_Day",
                       "Days_since_posted"]
    
    for col_name in numeric_cols:
        dt = dt.withColumn(col_name, col(col_name).cast("double"))
\end{lstlisting} 



\textbf{3.Variables catégorielles :} \\
Les variables catégorielles ont été encodées à l'aide d'un \texttt{StringIndexer} qui convertit chaque catégorie en indice numérique : 

\begin{itemize}
    \item \textbf{brand}: Marque du véhicule
    \item \textbf{condition}: État du véhicule
    \item \textbf{fuel\_type}: Type de carburant
    \item \textbf{model}: Modèle du véhicule
    \item \textbf{origin}: Origine du véhicule
    \item \textbf{first\_owner}: Premier propriétaire ou non
    \item \textbf{sector}: Secteur géographique
    \item \textbf{seller\_city}: Ville du vendeur
    \item \textbf{transmission}: Type de transmission
\end{itemize}


\begin{lstlisting}[style=pythonstyle]
# Categorical columns : 
    categorical_cols = ["brand", "condition", "fuel_type", "model", "origin", "first_owner",
                        "sector", "seller_city", "transmission"]
    
    dt = inplace_label_encode(dt, categorical_cols)
\end{lstlisting} 







\vspace{1mm}
\textbf{4.Traitement des valeurs aberrantes (outliers) :}\\
Pour améliorer la robustesse de nos modèles, nous avons détecté et éliminé les valeurs aberrantes en utilisant la méthode de l'écart interquartile (IQR) sur les variables suivantes :

\begin{itemize}
    \item \textbf{price}: Pour éliminer les prix anormalement bas ou élevés
    \item \textbf{mileage}: Pour exclure les kilométrages extrêmes
    \item \textbf{door\_count}: Pour corriger les erreurs potentielles sur le nombre de portes
    \item \textbf{fiscal\_power}: Pour supprimer les valeurs de puissance fiscale inhabituelles
\end{itemize}

\begin{lstlisting}[style=pythonstyle]
def remove_outliers_iqr(df, columns):
    cleaned_df = df.copy()
    for col in columns:
        Q1 = cleaned_df[col].quantile(0.25)
        Q3 = cleaned_df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        cleaned_df = cleaned_df[(cleaned_df[col] >= lower_bound) & (cleaned_df[col] <= upper_bound)]
    return cleaned_df
\end{lstlisting} 

Cette technique identifie comme aberrantes les valeurs situées à plus de 1,5 fois l'IQR au-dessous du premier quartile ou au-dessus du troisième quartile, assurant ainsi que nos modèles ne soient pas influencés par des observations exceptionnelles. \\




À la fin du processus de prétraitement, les données nettoyées et transformées ont été exportées dans un nouveau fichier CSV , prêt à être utilisé pour l'entraînement des différents modèles de prédiction. \\




\subsection{Présentation des métriques d'évaluation}
Dans le cadre de notre problème de régression pour la prédiction des prix de voitures, il est essentiel d'utiliser des métriques d'évaluation appropriées pour mesurer et comparer la performance des différents modèles. Nous avons sélectionné trois métriques complémentaires, chacune offrant une perspective différente sur la qualité des prédictions. \\

\subsubsection{Erreur quadratique moyenne (RMSE - Root Mean Square Error)}
L'erreur quadratique moyenne est notre principale métrique d'évaluation. Elle mesure l'écart moyen entre les prix prédits et les prix réels, tout en pénalisant davantage les erreurs importantes grâce à l'élévation au carré. \\

Mathématiquement, elle se calcule en prenant la racine carrée de la moyenne des carrés des différences entre les valeurs prédites et les valeurs réelles : \\

\[
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2}
\] \\

Une RMSE plus faible indique une meilleure performance du modèle. Cette métrique est particulièrement pertinente dans notre contexte car elle est exprimée dans la même unité que notre variable cible (en dirhams), ce qui la rend facilement interprétable.\\

De plus, sa sensibilité aux grandes erreurs reflète bien l'importance de minimiser les écarts significatifs dans l'estimation des prix de véhicules, où une erreur de prédiction importante peut avoir des conséquences considérables.

\subsubsection{Erreur absolue moyenne (MAE - Mean Absolute Error)}
L'erreur absolue moyenne calcule la moyenne des valeurs absolues des différences entre les prédictions et les valeurs réelles : \\

\[
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} \left| y_i - \hat{y}_i \right|
\] \\

Contrairement à la RMSE, cette métrique traite toutes les erreurs de manière proportionnelle, sans pénaliser davantage les grands écarts. \\

Exprimée également dans l'unité de la variable cible, la MAE nous fournit une indication plus intuitive de l'erreur moyenne à laquelle on peut s'attendre lors de l'utilisation du modèle.\\

Dans le contexte de la prédiction des prix de voitures, elle nous permet d'estimer directement la marge d'erreur moyenne en dirhams que pourrait commettre notre système.


\subsubsection{Coefficient de détermination (R²)}
Le coefficient de détermination, ou R², mesure la proportion de la variance dans la variable dépendante (le prix) qui est prévisible à partir des variables indépendantes (les caractéristiques du véhicule). Il se calcule selon la formule : \\

\[
R^2 = 1 - \frac{\sum_{i=1}^{n} \left( y_i - \bar{y} \right)^2}{\sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2}
\] \\

Où \( \bar{y} \) représente la moyenne des valeurs observées. Le \( R^2 \) prend des valeurs entre 0 et 1, où 1 indique une prédiction parfaite et 0 signifie que le modèle ne performe pas mieux qu'une simple prédiction par la moyenne. Cette métrique adimensionnelle nous offre une vision complémentaire de la qualité globale du modèle et de sa capacité à capturer la variabilité des prix sur le marché automobile.





\subsection{Modèle 1 : Random forest regressor}

\subsubsection{Architecture du modèle}

Le Random Forest Regressor est un algorithme d'ensemble qui combine plusieurs arbres de décision pour produire une prédiction plus précise et robuste. Chaque arbre est construit à partir d'un échantillon bootstrap des données d'entraînement (échantillonnage avec remplacement), et la prédiction finale est la moyenne des prédictions de tous les arbres.


\subsubsection{Description du modèle}
Un \textit{Random Forest Regressor} fonctionne comme suit :

\begin{enumerate}
  \item Création de multiples arbres de décision, chacun entraîné sur un sous-ensemble aléatoire des données (bootstrap)
  \item Pour chaque nœud des arbres, sélection d'un sous-ensemble aléatoire de caractéristiques
  \item Division optimale sur ces caractéristiques selon un critère comme la réduction de variance
  \item Répétition jusqu'à atteindre la profondeur maximale ou un autre critère d'arrêt
  \item Prédiction finale par moyenne des prédictions individuelles de tous les arbres
\end{enumerate}

\subsubsection{Justification du choix (avantages théoriques pour ce problème)}

Le \textit{Random Forest} présente plusieurs avantages pour les problèmes de régression :

\begin{enumerate}
  \item \textbf{Gestion naturelle des relations non linéaires} : peut capturer des relations complexes entre les variables prédictives et la variable cible.
  \item \textbf{Robustesse face au bruit} : l'utilisation de multiples arbres et l'échantillonnage bootstrap réduisent l'impact des valeurs aberrantes.
  \item \textbf{Faible risque de surapprentissage} : moins sujet au surapprentissage que les arbres de décision individuels.
  \item \textbf{Gestion efficace des données volumineuses} : peut traiter un grand nombre de variables sans sélection préalable.
  \item \textbf{Évaluation intégrée de l'importance des variables} : facilite l'interprétation des résultats.
\end{enumerate}






\subsubsection{Hyperparamètres}

Les principaux hyperparamètres du \texttt{RandomForestRegressor} de PySpark sont :

\begin{itemize}
  \item \texttt{numTrees} : Nombre d'arbres dans la forêt
  \item \texttt{maxDepth} : Profondeur maximale de chaque arbre
  \item \texttt{minInstancesPerNode} : Nombre minimum d'instances requises pour être à un nœud enfant
  \item \texttt{featureSubsetStrategy} : Stratégie de sélection des caractéristiques (par exemple, "auto", "all", "sqrt", "log2", "onethird")
  \item \texttt{impurity} : Critère utilisé pour l'information gain, comme "variance" pour la régression
  \item \texttt{maxBins} : Nombre maximum de bins pour discrétiser les caractéristiques continues
  \item \texttt{seed} : Graine aléatoire pour la reproductibilité
\end{itemize}

\subsubsection{Prapare data for model training}


\subsubsection{Stratégie de sélection (validation croisée, grid search)}
Pour optimiser les hyperparamètres du \texttt{RandomForestRegressor}, nous avons utilisé les approches suivantes :

\begin{itemize}
  \item \textbf{Validation croisée à 5 plis} : Permet d’évaluer la performance du modèle de manière robuste tout en réduisant la variance de l’estimation.
  \item \textbf{GridSearchCV} : Procédure de recherche exhaustive sur un espace de combinaisons d’hyperparamètres prédéfinies.
  \item \textbf{Métrique d’évaluation} : L'erreur quadratique moyenne (RMSE) a été utilisée pour mesurer la qualité des prédictions.
\end{itemize}

La grille de recherche utilisée est la suivante : \\

\begin{lstlisting}[style=pythonstyle]

# Split data into train and test : 
train_data, test_data = data_for_training.randomSplit([0.8, 0.2], seed=42)

# 4. Set up base model
rf = RandomForestRegressor(
    featuresCol="features", 
    labelCol="price",
    predictionCol="prediction"
)

# Define parameter grid
paramGrid = ParamGridBuilder() \
    .addGrid(rf.maxDepth, [5, 10]) \
    .addGrid(rf.minInstancesPerNode, [5, 10]) \
    .addGrid(rf.numTrees, [100, 200]) \
    .addGrid(rf.subsamplingRate, [0.8, 1.0]) \
    .addGrid(rf.featureSubsetStrategy, ["onethird", "sqrt"]) \
    .build()  

# Set up evaluator
evaluator = RegressionEvaluator(
    labelCol="price", 
    predictionCol="prediction", 
    metricName="rmse"
)

# Create cross-validator 
cv = CrossValidator(
    estimator=rf,
    estimatorParamMaps=paramGrid,
    evaluator=evaluator,
    numFolds=3,  # Fewer folds for faster computation
    parallelism=2,  # Control parallelism based on your CPU
    seed=42
)

\end{lstlisting}


\subsubsection{Entraînement}
\begin{lstlisting}[style=pythonstyle]
# Fit cross-validator to find best model
cv_model = cv.fit(train_data)

# Get best model
best_model = cv_model.bestModel
\end{lstlisting}


\subsubsection{Paramètres optimaux retenus}

Après l'optimisation par \texttt{CrossValidator}, les meilleurs hyperparamètres sélectionnés sont les suivants :

\begin{lstlisting}[style=pythonstyle]
print("Best Model Parameters:")
print(f"Max Depth: {best_model._java_obj.getMaxDepth()}")
print(f"Min Instances Per Node: {best_model._java_obj.getMinInstancesPerNode()}")
print(f"Number of Trees: {best_model._java_obj.getNumTrees()}")
print(f"Subsampling Rate: {best_model._java_obj.getSubsamplingRate()}")
print(f"Feature Subset Strategy: {best_model._java_obj.getFeatureSubsetStrategy()}")
\end{lstlisting}


\begin{itemize}
  \item \texttt{maxDepth} : 10
  \item \texttt{minInstancesPerNode} : 5
  \item \texttt{numTrees} : 200
  \item \texttt{subsamplingRate} : 0.8
  \item \texttt{featureSubsetStrategy} : \texttt{sqrt}
\end{itemize}


Ces paramètres offrent le meilleur compromis entre biais et variance, permettant au modèle de généraliser efficacement sur de nouvelles données tout en capturant les relations complexes présentes dans l'ensemble d'entraînement.






\subsubsection{Évaluation du modèle}

L'évaluation du modèle a été réalisée à la fois sur les données d'entraînement et de test, en utilisant plusieurs métriques pour mesurer sa performance prédictive. \\

\begin{lstlisting}[style=pythonstyle]
# Évaluation sur les données de test
test_predictions = best_model.transform(test_data)
rmse_test = evaluator.evaluate(test_predictions)
mae_test = evaluator.evaluate(test_predictions, {evaluator.metricName: "mae"})
r2_test = evaluator.evaluate(test_predictions, {evaluator.metricName: "r2"})
print(f"Test data: {{'rmse': {rmse_test}, 'mae': {mae_test}, 'r2': {r2_test}}}")

# Évaluation sur les donnees d'entraînement
train_predictions = best_model.transform(train_data)
rmse_train = evaluator.evaluate(train_predictions)
mae_train = evaluator.evaluate(train_predictions, {evaluator.metricName: "mae"})
r2_train = evaluator.evaluate(train_predictions, {evaluator.metricName: "r2"})
print(f"Train data: {{'rmse': {rmse_train}, 'mae': {mae_train}, 'r2': {r2_train}}}")
\end{lstlisting}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Données} & \textbf{RMSE} & \textbf{MAE} & \textbf{R\textsuperscript{2}} \\
\hline
Entraînement & 49\,237.44 & 27\,100.66 & 0.828 \\
\hline
Test         & 68\,148.93 & 33\,172.46 & 0.696 \\
\hline
\end{tabular}
\caption{Performances du modèle Random Forest sur les jeux d'entraînement et de test}
\label{tab:rf_performance}
\end{table}



\subsubsection{Interprétation des résultats}







\subsection{Modèle 2 : light GBM}
\subsubsection{Architecture du modèle}
\subsubsection{Description du modèle}
Justification du choix (avantages théoriques pour ce problème)
Forces et faiblesses attendues
\subsubsection{Hyperparamètres}
Stratégie de sélection (validation croisée, grid search)
Paramètres optimaux retenus
\subsubsection{Entraînement}
Configuration et approche
Défis rencontrés



\subsection{Modèle 3 : Xgboost}


\subsubsection{Architecture du modèle}

Le XGBoost Regressor est un algorithme d'ensemble basé sur le boosting de gradient, qui combine plusieurs arbres de décision pour produire une prédiction plus précise et robuste. Chaque arbre est construit pour corriger les erreurs des précédents en minimisant une fonction de perte, et la prédiction finale est la somme des contributions de tous les arbres, ajustée par un taux d'apprentissage.

\subsubsection{Description du modèle}

Un \textit{XGBoost Regressor} fonctionne comme suit :

\begin{enumerate}
  \item Initialisation avec une prédiction de base, généralement la moyenne des valeurs cibles (ici, le prix moyen des véhicules dans l’ensemble d’entraînement, ce qui fournit un point de départ simple pour les itérations suivantes).
  \item Construction itérative d'arbres de décision :
        \begin{itemize}
            \item Calcul des gradients et Hessians pour minimiser la fonction de perte (\texttt{reg:squarederror}), qui mesure l’écart quadratique entre les prédictions et les vraies valeurs des prix.
            \item Construction d’un arbre pour ajuster les résidus (erreurs) des prédictions précédentes, en utilisant un sous-ensemble aléatoire des données (via \texttt{subsample}) et des caractéristiques (via \texttt{colsample\_bytree}) pour introduire de la variabilité et éviter le surapprentissage.
            \item Application de régularisations (L1 via \texttt{reg\_alpha} et L2 via \texttt{reg\_lambda}) pour limiter la complexité des arbres, ce qui est crucial pour un jeu de données avec 37 caractéristiques incluant des variables numériques (\texttt{mileage}, \texttt{fiscal\_power}), catégoriques (\texttt{brand}, \texttt{fuel\_type}) et binaires (\texttt{Sunroof}, \texttt{Alloy\_wheels}).
        \end{itemize}
  \item Mise à jour des prédictions en ajoutant l’arbre, pondéré par un taux d’apprentissage (\texttt{learning\_rate}), ce qui permet de contrôler la vitesse de correction des erreurs et d’assurer une convergence stable.
  \item Répétition jusqu’à atteindre le nombre maximum d’arbres (\texttt{n\_estimators}) ou un critère d’arrêt anticipé, comme l’absence d’amélioration sur un ensemble de validation après 20 itérations.
  \item Prédiction finale par somme des prédictions individuelles de tous les arbres, ce qui donne une estimation robuste du prix en combinant les contributions de chaque arbre.
\end{enumerate}

\subsubsection{Justification du choix (avantages théoriques pour ce problème)}

Le \textit{XGBoost Regressor} présente plusieurs avantages pour les problèmes de régression, en particulier pour la prédiction des prix de véhicules :

\begin{enumerate}
  \item \textbf{Gestion naturelle des relations non linéaires} : peut capturer des interactions complexes entre les variables, comme l’impact combiné du kilométrage (\texttt{mileage}) et de l’âge du véhicule (\texttt{vehicle\_age}) sur le prix, où une augmentation du kilométrage pourrait avoir un effet plus prononcé sur les véhicules plus anciens.
  \item \textbf{Robustesse face aux valeurs aberrantes} : les régularisations et le boosting réduisent l’impact des anomalies, comme des prix extrêmes pour des véhicules de luxe (par exemple, une voiture de marque premium avec un prix très élevé) ou des véhicules très anciens à bas prix, ce qui est probable dans un jeu de données comme \texttt{data\_preprocessed\_V3.csv}.
  \item \textbf{Flexibilité avec les données hétérogènes} : gère efficacement les 37 caractéristiques du jeu de données, incluant des variables numériques continues (\texttt{mileage}), catégoriques (\texttt{brand}, avec plusieurs niveaux possibles), et binaires (\texttt{Sunroof}), après leur encodage dans le prétraitement.
  \item \textbf{Évaluation intégrée de l’importance des variables} : fournit des scores d’importance pour identifier les caractéristiques clés influençant le prix, comme le kilométrage ou la marque, ce qui est essentiel pour comprendre les facteurs déterminants dans un système de recommandation de véhicules.
  \item \textbf{Scalabilité} : performant sur des ensembles de données volumineux avec de nombreuses variables, ce qui est adapté à notre jeu de données comportant 37 colonnes et potentiellement un grand nombre d’observations.
\end{enumerate}

\subsubsection{Forces et faiblesses attendues}

\textbf{Forces}
\begin{itemize}
  \item Excellente précision grâce au boosting itératif, qui corrige progressivement les erreurs et capture des relations complexes, comme l’interaction entre \texttt{brand} et \texttt{Leather\_seats} pour prédire les prix des véhicules de luxe.
  \item Robustesse aux valeurs aberrantes et au bruit dans les données, par exemple, un véhicule avec un kilométrage exceptionnellement élevé ou un prix anormalement bas, grâce à la régularisation et au sous-échantillonnage.
  \item Capacité à gérer des données manquantes après prétraitement, ce qui est utile si certaines caractéristiques comme \texttt{fuel\_type} ou \texttt{mileage} présentent des valeurs manquantes dans le jeu de données initial.
  \item Pas besoin de normaliser les données, ce qui simplifie le pipeline de prétraitement pour des variables comme \texttt{mileage} ou \texttt{fiscal\_power}, qui peuvent avoir des échelles très différentes.
  \item Parallélisation facile pour accélérer l’entraînement, ce qui est crucial pour un modèle avec 990 arbres et un jeu de données potentiellement volumineux.
\end{itemize}

\textbf{Faiblesses}
\begin{itemize}
  \item Moins interprétable qu’un arbre de décision unique ou une régression linéaire, car les prédictions reposent sur la combinaison de centaines d’arbres, rendant difficile l’explication des contributions individuelles des 37 caractéristiques.
  \item Sensibilité aux hyperparamètres, nécessitant un réglage approfondi ; par exemple, un \texttt{learning\_rate} trop élevé pourrait entraîner une convergence instable, tandis qu’un \texttt{max\_depth} excessif pourrait causer du surapprentissage.
  \item Performances potentiellement réduites sur des données très bruitées sans prétraitement adéquat, comme des erreurs dans l’encodage des variables catégoriques (\texttt{brand}) ou des valeurs aberrantes non traitées dans \texttt{price}.
  \item Temps d’entraînement et utilisation mémoire plus importants que pour des modèles plus simples, notamment avec \texttt{n\_estimators=990} et une validation croisée à 5 plis, ce qui peut poser problème sur des machines aux ressources limitées.
  \item Moins adapté aux problèmes fortement linéaires, où une régression linéaire serait plus appropriée ; par exemple, si le prix dépendait principalement d’une relation linéaire simple avec \texttt{vehicle\_age}, XGBoost pourrait être excessif.
\end{itemize}

\subsubsection{Hyperparamètres}

Les principaux hyperparamètres du \texttt{XGBRegressor} sont :

\begin{itemize}
  \item \texttt{n\_estimators} : Nombre d’arbres dans l’ensemble, contrôlant la capacité du modèle ; une valeur élevée (comme 990) augmente la précision mais aussi le risque de surapprentissage.
  \item \texttt{max\_depth} : Profondeur maximale de chaque arbre, limitant la complexité ; une valeur de 6 permet de capturer des interactions sans trop surajuster.
  \item \texttt{min\_child\_weight} : Poids minimum requis pour créer un nœud enfant, influençant la robustesse aux petites variations dans les données.
  \item \texttt{gamma} : Réduction minimale de la perte pour effectuer une division, agissant comme une régularisation supplémentaire pour éviter des divisions inutiles.
  \item \texttt{subsample} : Fraction des échantillons utilisée pour chaque arbre, introduisant de la variabilité pour réduire le surapprentissage.
  \item \texttt{colsample\_bytree} : Fraction des caractéristiques utilisées pour chaque arbre, permettant de diversifier les arbres et d’éviter la dépendance excessive à certaines variables comme \texttt{mileage}.
  \item \texttt{learning\_rate} : Taux d’apprentissage pour les mises à jour, contrôlant la vitesse de convergence ; une valeur faible (0,027) assure une amélioration progressive.
  \item \texttt{reg\_alpha} : Régularisation L1, pénalisant les poids pour réduire la complexité et gérer les variables moins pertinentes.
  \item \texttt{reg\_lambda} : Régularisation L2, favorisant des arbres plus simples pour améliorer la généralisation.
  \item \texttt{random\_state} : Graine aléatoire pour la reproductibilité, garantissant des résultats cohérents entre les exécutions.
\end{itemize}

\subsubsection{Stratégie de sélection (validation croisée, grid search)}

Pour optimiser les hyperparamètres du \texttt{XGBRegressor}, nous avons utilisé les approches suivantes :

\begin{itemize}
  \item \textbf{Validation croisée à 5 plis} : Permet d’évaluer la performance du modèle de manière robuste tout en réduisant la variance de l’estimation, en divisant les données en 5 sous-ensembles pour tester la généralisation sur différentes portions.
  \item \textbf{RandomizedSearchCV} : Procédure de recherche aléatoire sur un espace de distributions d’hyperparamètres prédéfini, plus efficace qu’une recherche exhaustive (\texttt{GridSearchCV}) pour un grand nombre de paramètres comme ici (9 paramètres avec des plages larges), car elle teste 100 combinaisons au lieu de toutes les possibilités.
  \item \textbf{Métrique d’évaluation} : L’erreur quadratique moyenne (MSE) a été utilisée pour mesurer la qualité des prédictions, car elle pénalise davantage les grandes erreurs, ce qui est pertinent pour des prédictions de prix où les écarts importants peuvent avoir un impact significatif.
\end{itemize}

La grille de recherche utilisée est la suivante : \\

\begin{lstlisting}[style=pythonstyle]
param_dist = {
    'n_estimators': randint(100, 1000),
    'max_depth': randint(3, 10),
    'min_child_weight': randint(1, 10),
    'gamma': uniform(0, 0.5),
    'subsample': uniform(0.6, 0.4),
    'colsample_bytree': uniform(0.6, 0.4),
    'learning_rate': uniform(0.01, 0.19),
    'reg_alpha': uniform(0, 10),
    'reg_lambda': uniform(0, 10)
}

random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_dist,
    n_iter=100,
    scoring='neg_mean_squared_error',
    cv=KFold(n_splits=5, shuffle=True, random_state=42),
    verbose=1,
    n_jobs=-1,
    random_state=42
)
\end{lstlisting}

\subsubsection{Entraînement}

L’entraînement a été effectué avec un \texttt{random\_state=42} pour garantir la reproductibilité. Le jeu de données a été divisé en trois ensembles : 64~\% pour l’entraînement, 16~\% pour la validation (utilisée pour l’arrêt anticipé), et 20~\% pour le test. Cette répartition permet d’avoir suffisamment de données pour entraîner le modèle tout en réservant une partie pour évaluer la généralisation. De plus, un arrêt anticipé (\texttt{early\_stopping\_rounds=20}) a été appliqué pour stopper l’entraînement si la performance sur l’ensemble de validation ne s’améliore pas après 20 itérations, ce qui a aidé à réduire le surapprentissage, comme indiqué par la différence entre les R² d’entraînement (0,904) et de test (0,787).

\begin{lstlisting}[style=pythonstyle]
# Ajustement de RandomizedSearchCV pour trouver le meilleur modèle
random_search.fit(X_train, y_train)

# Récupération du meilleur modèle
best_model = random_search.best_estimator_
\end{lstlisting}

\subsubsection{Paramètres optimaux retenus}

Après l’optimisation par \texttt{RandomizedSearchCV}, les meilleurs hyperparamètres sélectionnés sont les suivants :

\begin{lstlisting}[style=pythonstyle]
print("Best Model Parameters:")
print(f"colsample_bytree: {best_model.colsample_bytree}")
print(f"gamma: {best_model.gamma}")
print(f"learning_rate: {best_model.learning_rate}")
print(f"max_depth: {best_model.max_depth}")
print(f"min_child_weight: {best_model.min_child_weight}")
print(f"n_estimators: {best_model.n_estimators}")
print(f"reg_alpha: {best_model.reg_alpha}")
print(f"reg_lambda: {best_model.reg_lambda}")
print(f"subsample: {best_model.subsample}")
\end{lstlisting}

\begin{itemize}
  \item \texttt{colsample\_bytree} : 0,797
  \item \texttt{gamma} : 0,042
  \item \texttt{learning\_rate} : 0,027
  \item \texttt{max\_depth} : 6
  \item \texttt{min\_child\_weight} : 9
  \item \texttt{n\_estimators} : 990
  \item \texttt{reg\_alpha} : 2,127
  \item \texttt{reg\_lambda} : 9,462
  \item \texttt{subsample} : 0,913
\end{itemize}

Ces paramètres offrent le meilleur compromis entre biais et variance, permettant au modèle de généraliser efficacement sur de nouvelles données tout en capturant les relations complexes présentes dans l’ensemble d’entraînement. Par exemple, un \texttt{learning\_rate} bas (0,027) combiné à un grand nombre d’arbres (990) permet une convergence progressive, tandis que des régularisations élevées (\texttt{reg\_alpha}, \texttt{reg\_lambda}) limitent le surapprentissage.

\subsubsection{Évaluation du modèle}

L’évaluation du modèle a été réalisée à la fois sur les données d’entraînement et de test, en utilisant plusieurs métriques pour mesurer sa performance prédictive. Le RMSE (racine de l’erreur quadratique moyenne) mesure l’erreur moyenne des prédictions en unités de prix, le MAE (erreur absolue moyenne) donne une mesure plus intuitive de l’erreur moyenne sans pénaliser autant les grandes erreurs, et le R² (coefficient de détermination) indique la proportion de variance des prix expliquée par le modèle.

\begin{lstlisting}[style=pythonstyle]
# Évaluation sur les données de test
y_test_pred = best_model.predict(X_test)
mse_test = mean_squared_error(y_test, y_test_pred)
rmse_test = np.sqrt(mse_test)
mae_test = mean_absolute_error(y_test, y_test_pred)
r2_test = r2_score(y_test, y_test_pred)
print(f"Test data: {{'mse': {mse_test}, 'rmse': {rmse_test}, 'mae': {mae_test}, 'r2': {r2_test}}}")

# Évaluation sur les données d’entraînement
y_train_pred = best_model.predict(X_train)
mse_train = mean_squared_error(y_train, y_train_pred)
rmse_train = np.sqrt(mse_train)
mae_train = mean_absolute_error(y_train, y_train_pred)
r2_train = r2_score(y_train, y_train_pred)
print(f"Train data: {{'mse': {mse_train}, 'rmse': {rmse_train}, 'mae': {mae_train}, 'r2': {r2_train}}}")
\end{lstlisting}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Données} & \textbf{RMSE} & \textbf{MAE} & \textbf{R\textsuperscript{2}} \\
\hline
Entraînement & 48\,214.32 & 29\,528.51 & 0.904 \\
\hline
Test         & 55\,673.00 & 34\,068.53 & 0.787 \\
\hline
\end{tabular}
\caption{Performances du modèle XGBoost sur les jeux d’entraînement et de test}
\label{tab:xgboost_performance}
\end{table}

Sur l’ensemble de test, un RMSE de 55\,673 signifie que, en moyenne, les prédictions s’écartent de la vraie valeur du prix d’environ 55\,673 unités monétaires (par exemple, euros), ce qui est acceptable pour des prix de véhicules pouvant varier de dizaines à centaines de milliers. Le MAE de 34\,068.53 indique une erreur absolue moyenne plus faible, montrant que les erreurs importantes sont rares. Le R² de 0,787 montre que le modèle explique 78,7~\% de la variance des prix, ce qui est une bonne performance pour un système de recommandation. Sur l’ensemble d’entraînement, le R² plus élevé (0,904) et le RMSE plus bas (48\,214.32) indiquent une bonne adaptation aux données d’entraînement, mais la différence avec les métriques de test suggère un léger surapprentissage, qui pourrait être atténué avec un prétraitement supplémentaire ou un réglage plus fin des hyperparamètres.






\subsection{Modèle 4 : Réseau de neurones artificiels}

\subsubsection{Architecture du modèle}

Le Réseau de neurones artificiels (RNA) est un modèle d’apprentissage profond qui utilise des couches de neurones interconnectés pour produire des prédictions précises et robustes, en s’appuyant sur une architecture multicouche optimisée pour la régression des prix de véhicules, avec une stratégie d’ensemble pour améliorer la généralisation.

\subsubsection{Description du modèle}

Un RNA fonctionne comme suit :

\begin{enumerate}
  \item Les données d’entrée (36 caractéristiques comme \texttt{mileage}, \texttt{brand}, et \texttt{Sunroof}) sont transformées via une couche d’entrée et propagées à travers plusieurs couches cachées.
  \item Chaque neurone dans les couches cachées applique une fonction d’activation (ici, LeakyReLU) pour introduire de la non-linéarité, permettant au modèle de capturer des relations complexes entre les variables et le prix.
  \item Les prédictions sont générées par une couche de sortie avec une activation linéaire, produisant une valeur continue pour le prix.
  \item L’entraînement ajuste les poids des connexions en minimisant une fonction de perte (ici, Huber loss) via une optimisation par descente de gradient, avec des techniques comme l’arrêt anticipé et la réduction du taux d’apprentissage, complétées par une approche d’ensemble.
\end{enumerate}

\subsubsection{Composition de l’architecture}

L’architecture du RNA est composée de plusieurs couches soigneusement conçues :

\begin{enumerate}
  \item \textbf{Couche d’entrée} : Accepte les 36 caractéristiques normalisées (via RobustScaler) pour traiter les données hétérogènes (numériques, catégoriques encodées).
  \item \textbf{Couches cachées} :
        \begin{itemize}
            \item Première couche : 64 neurones avec LeakyReLU, BatchNormalization pour stabiliser l’entraînement, Dropout (0.3) pour prévenir le surapprentissage, et régularisation L1/L2 pour contrôler la complexité.
            \item Deuxième couche : 32 neurones avec les mêmes mécanismes, réduisant la dimensionnalité tout en conservant les informations pertinentes.
            \item Troisième couche : 16 neurones, avec un Dropout réduit (0.2) pour ajuster la régularisation.
            \item Quatrième couche : 8 neurones, ajoutée pour augmenter la capacité du modèle à capturer des relations subtiles dans les données.
        \end{itemize}
  \item \textbf{Couche de sortie} : 1 neurone avec activation linéaire pour produire la prédiction du prix.
\end{enumerate}

Cette architecture multicouche, avec une réduction progressive du nombre de neurones, permet de transformer les caractéristiques brutes en une prédiction cohérente tout en évitant le surapprentissage.

\subsubsection{Raisonnement derrière l’architecture}

Le choix de cette architecture repose sur plusieurs considérations :

\begin{enumerate}
  \item \textbf{Profondeur et largeur} : Quatre couches cachées avec un nombre décroissant de neurones (64, 32, 16, 8) augmentent la capacité du modèle à apprendre des relations non linéaires complexes entre les 36 caractéristiques (par exemple, l’interaction entre \texttt{mileage} et \texttt{vehicle\_age}), tout en réduisant le risque de surapprentissage grâce à la diminution progressive.
  \item \textbf{LeakyReLU} : Utilisée comme fonction d’activation pour éviter le problème des neurones "morts" (valeurs négatives bloquées à zéro), ce qui est pertinent pour des données contenant des valeurs aberrantes comme les prix extrêmes.
  \item \textbf{BatchNormalization} : Ajoutée après chaque couche pour normaliser les entrées des neurones, accélérant la convergence et rendant le modèle plus stable, surtout avec un grand nombre d’époques (jusqu’à 500) sur 14\,416 échantillons.
  \item \textbf{Dropout} : Appliqué avec des taux décroissants (0.3, 0.3, 0.2, 0.1) pour introduire de la régularisation et empêcher le modèle de dépendre trop d’une seule caractéristique, comme \texttt{brand} ou \texttt{fiscal\_power}, en désactivant aléatoirement des neurones pendant l’entraînement.
  \item \textbf{Régularisation L1/L2} : Utilisée pour pénaliser les poids excessifs, ce qui est crucial pour un modèle avec 36 caractéristiques potentiellement corrélées, réduisant ainsi le risque de surapprentissage.
  \item \textbf{Couche supplémentaire} : L’ajout d’une quatrième couche (8 neurones) vise à améliorer la capacité du modèle à capturer des motifs subtils, comme les effets combinés de caractéristiques binaires (\texttt{Sunroof}, \texttt{Alloy\_wheels}) sur le prix.
  \item \textbf{Optimiseur Adam} : Employé avec un taux d’apprentissage initial de 0.0005, il adapte dynamiquement les taux d’apprentissage pour chaque paramètre en utilisant des estimations de premier et second moment (momentum et RMSProp), ce qui accélère la convergence et améliore la performance sur un jeu de données hétérogène comme celui-ci.
\end{enumerate}

\subsubsection{Justification du choix (avantages théoriques pour ce problème)}

Le RNA présente plusieurs avantages pour la prédiction des prix de véhicules :

\begin{enumerate}
  \item \textbf{Capture des relations non linéaires} : Les couches cachées avec LeakyReLU et une architecture profonde permettent de modéliser des interactions complexes, comme l’effet non linéaire du kilométrage sur les prix pour différentes marques.
  \item \textbf{Robustesse aux données bruitées} : L’utilisation de RobustScaler, Huber loss (moins sensible aux outliers), et l’approche d’ensemble conviennent à un jeu de données avec des prix extrêmes (par exemple, véhicules de luxe ou très anciens).
  \item \textbf{Flexibilité avec les données hétérogènes} : Les 36 caractéristiques (numériques, catégoriques encodées) sont bien traitées après normalisation, permettant au modèle de tirer parti de toutes les informations disponibles.
  \item \textbf{Adaptabilité via hyperparamètres} : Les techniques comme Dropout, BatchNormalization, et la réduction du taux d’apprentissage permettent d’ajuster le modèle pour éviter le surapprentissage sur 14\,416 échantillons.
  \item \textbf{Potentiel d’amélioration} : L’approche d’ensemble avec des poids basés sur la performance valide cette architecture comme une base solide pour des prédictions précises.
\end{enumerate}

\subsubsection{Forces et faiblesses attendues}

\textbf{Forces}
\begin{itemize}
  \item Excellente capacité à modéliser des relations complexes grâce à une architecture profonde et des activations non linéaires.
  \item Robustesse aux valeurs aberrantes grâce à RobustScaler, Huber loss, et l’ensemble, idéal pour des prix atypiques les données.
  \item Capacité à généraliser avec Dropout, BatchNormalization, et régularisation, même sur un grand nombre de caractéristiques (36).
  \item Pas besoin de sélection manuelle des caractéristiques, car le modèle apprend automatiquement leur importance.
  \item Parallélisation possible via Spark pour gérer les 14\,416 échantillons efficacement.
\end{itemize}

\textbf{Faiblesses}
\begin{itemize}
  \item Moins interprétable qu’un modèle linéaire ou un arbre de décision, rendant difficile l’explication des contributions des 36 caractéristiques.
  \item Sensibilité aux hyperparamètres et à la qualité des données ; un mauvais réglage de Dropout ou un bruit dans \texttt{brand} peut affecter les résultats.
  \item Temps d’entraînement élevé avec 500 époques et 14\,416 échantillons, même avec Spark et BatchNormalization.
  \item Risque de surapprentissage si les données ne sont pas bien prétraitées (par exemple, outliers non gérés dans \texttt{mileage}), bien que mitigé par les outils d’optimisation.
  \item Moins adapté à des relations fortement linéaires, où une régression linéaire serait plus efficace.
\end{itemize}

\subsubsection{Préparation des données}

Avant l’entraînement, les données sont préparées pour optimiser les performances du RNA :

\begin{itemize}
  \item \textbf{Chargement} : Les données sont lues depuis \texttt via Spark, avec 14\,416 échantillons et 36 caractéristiques plus la cible \texttt{price}.
  \item \textbf{Normalisation} : RobustScaler est utilisé pour normaliser les caractéristiques et la cible, rendant le modèle moins sensible aux outliers (par exemple, prix élevés ou kilométrages extrêmes), en éliminant les effets des valeurs médianes et quartiles.
  \item \textbf{Division} : Une validation croisée à 5 plis est appliquée pour évaluer la robustesse, avec un mélange aléatoire des données.
\end{itemize}

\subsubsection{Explication du code}

Le code de préparation charge les données avec Spark, convertit en Pandas pour la compatibilité avec TensorFlow, et applique RobustScaler pour normaliser les 36 caractéristiques et la cible \texttt{price}.

\begin{lstlisting}[style=pythonstyle]
# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Start Spark session
spark = SparkSession.builder.appName("ImprovedNeuralNetwork").getOrCreate()

# Load data
df = spark.read.csv("data_preprocessed_V3.csv", header=True, inferSchema=True)

# Convert to Pandas right away
pandas_df = df.toPandas()

# Separate features and target
X = pandas_df.drop('price', axis=1)
y = pandas_df['price']

# Scale features using RobustScaler
feature_scaler = RobustScaler()
X_scaled = feature_scaler.fit_transform(X)

# Scale target variable
target_scaler = RobustScaler()
y_scaled = target_scaler.fit_transform(y.values.reshape(-1, 1)).flatten()
\end{lstlisting}

\subsubsection{Hyperparamètres}

Les principaux hyperparamètres du RNA incluent :

\begin{itemize}
  \item \texttt{units} : Nombre de neurones par couche (64, 32, 16, 8), contrôlant la capacité du modèle.
  \item \texttt{Dropout} : Taux de désactivation (0.3, 0.3, 0.2, 0.1), régulant le surapprentissage.
  \item \texttt{learning\_rate} : Taux d’apprentissage initial (0.0005) pour l’optimiseur Adam.
  \item \texttt{batch\_size} : Taille des lots (32), influençant la généralisation.
  \item \texttt{epochs} : Nombre maximum d’itérations (500), avec arrêt anticipé.
  \item \texttt{l1\_l2} : Régularisation (0.0001, 0.0005) pour limiter la complexité.
  \item \texttt{patience} : Nombre d’itérations sans amélioration pour l’arrêt anticipé (30) et la réduction du taux (10).
\end{itemize}

\subsubsection{Stratégie de sélection (validation croisée, grid search)}

La sélection des hyperparamètres repose sur :

\begin{itemize}
  \item \textbf{Validation croisée à 5 plis} : Évalue la performance sur différents sous-ensembles, réduisant le biais d’une seule division.
  \item \textbf{Ajustement manuel} : Les hyperparamètres sont ajustés manuellement en fonction des résultats de validation, avec des callbacks pour optimiser dynamiquement.
  \item \textbf{Métrique d’évaluation} : La perte Huber est utilisée pour minimiser l’impact des outliers dans les prédictions de prix.
\end{itemize}

\subsubsection{Explication du code}

Le code définit l’architecture du modèle avec des couches successives et configure les callbacks pour l’entraînement, utilisant Adam comme optimiseur adaptatif.

\begin{lstlisting}[style=pythonstyle]
def create_improved_model(input_dim):
    model = Sequential([
        Dense(64, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0005), input_shape=(input_dim,)),
        LeakyReLU(alpha=0.1),
        BatchNormalization(),
        Dropout(0.3),
        Dense(32, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0005)),
        LeakyReLU(alpha=0.1),
        BatchNormalization(),
        Dropout(0.3),
        Dense(16, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0005)),
        LeakyReLU(alpha=0.1),
        BatchNormalization(),
        Dropout(0.2),
        Dense(8, kernel_initializer='he_normal', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0005)),
        LeakyReLU(alpha=0.1),
        BatchNormalization(),
        Dropout(0.1),
        Dense(1)
    ])
    optimizer = Adam(learning_rate=0.0005)
    model.compile(optimizer=optimizer, loss='huber')
    return model
\end{lstlisting}

\subsubsection{Entraînement}

L’entraînement utilise une validation croisée à 5 plis, avec des outils pour optimiser les performances et prévenir le surapprentissage. Les données sont divisées en 5 plis (80~\% entraînement, 20~\% validation par pli), et un modèle est entraîné pour chaque pli avec un arrêt anticipé après 30 itérations sans amélioration, une réduction du taux d’apprentissage si la perte de validation stagne, et un sauvegarde du meilleur modèle.

\begin{lstlisting}[style=pythonstyle]
print(f"Training {k_folds} models with cross-validation...")

for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled)):
    print(f"\nFold {fold+1}/{k_folds}")
    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
    y_train, y_val = y_scaled[train_idx], y_scaled[val_idx]
    model = create_improved_model(X_scaled.shape[1])
    early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True, verbose=1)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.000005, verbose=1)
    model_checkpoint = ModelCheckpoint(f'best_model_fold_{fold+1}.h5', monitor='val_loss', save_best_only=True, verbose=1)
    history = model.fit(X_train, y_train, epochs=500, batch_size=32, validation_data=(X_val, y_val),
                        callbacks=[early_stopping, reduce_lr, model_checkpoint], verbose=1)
    model = load_model(f'best_model_fold_{fold+1}.h5')
    fold_models.append(model)
    y_pred_scaled = model.predict(X_val)
    y_pred = target_scaler.inverse_transform(y_pred_scaled)
    y_true = target_scaler.inverse_transform(y_val.reshape(-1, 1))
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    r2_scores.append(r2)
    rmse_scores.append(rmse)
    mae_scores.append(mae)
    fold_predictions.append(y_pred)
    fold_actuals.append(y_true)
    print(f"Fold {fold+1} - RMSE: {rmse:.2f}, MAE: {mae:.2f}, R²: {r2:.4f}")
\end{lstlisting}

\subsubsection{Explication du code}

La boucle entraîne un modèle par pli, utilisant Adam comme optimiseur pour ajuster les poids efficacement. EarlyStopping surveille la perte de validation et arrête après 30 itérations sans amélioration, restaurant les meilleurs poids pour éviter le surapprentissage. ReduceLROnPlateau réduit le taux d’apprentissage (facteur 0.2) après 10 itérations stagnantes, avec un minimum de 0.000005, pour affiner la convergence. ModelCheckpoint sauvegarde le meilleur modèle par pli, améliorant la performance globale.

\subsubsection{Paramètres optimaux retenus}

Les hyperparamètres optimaux sont dérivés des meilleurs modèles par pli :

\begin{itemize}
  \item \texttt{units} : [64, 32, 16, 8]
  \item \texttt{Dropout} : [0.3, 0.3, 0.2, 0.1]
  \item \texttt{learning\_rate} : 0.0005
  \item \texttt{batch\_size} : 32
  \item \texttt{epochs} : Jusqu’à 500, avec arrêt anticipé à environ 30-40 itérations par pli.
  \item \texttt{l1\_l2} : (0.0001, 0.0005)
  \item \texttt{patience} : 30 (early stopping), 10 (reduce LR)
\end{itemize}

\subsubsection{Utilisation de l’apprentissage d’ensemble}

L’apprentissage d’ensemble combine les prédictions des 5 modèles entraînés sur les plis de validation, utilisant des poids basés sur l’inverse des RMSE de validation. Cette approche améliore la généralisation en réduisant la variance des prédictions individuelles, exploitant la diversité des modèles entraînés sur des sous-ensembles différents des 14\,416 échantillons.

\begin{lstlisting}[style=pythonstyle]
def ensemble_predict(models, X_data, weights=None):
    predictions = np.array([model.predict(X_data).flatten() for model in models])
    if weights is None:
        weights = np.ones(len(models)) / len(models)
    weighted_predictions = np.sum(predictions.T * weights, axis=1)
    return weighted_predictions.reshape(-1, 1)

inverse_rmse = [1/score for score in rmse_scores]
weights = np.array(inverse_rmse) / sum(inverse_rmse)
print(f"Ensemble weights based on validation performance: {weights}")
\end{lstlisting}

\subsubsection{Explication du code}

La fonction \texttt{ensemble\_predict} calcule une prédiction pondérée, où les poids sont dérivés de l’inverse des RMSE, favorisant les modèles les plus précis. Cela combine les prédictions des 5 plis, réduisant les erreurs et améliorant la robustesse sur les 36 caractéristiques.

\subsubsection{Évaluation du modèle}

Les performances sont évaluées sur les plis de validation et l’ensemble final.

\begin{lstlisting}[style=pythonstyle]
all_preds = []
all_actuals = []

for i, (_, val_idx) in enumerate(kf.split(X_scaled)):
    X_val = X_scaled[val_idx]
    y_val = y_scaled[val_idx]
    y_pred_scaled = ensemble_predict([fold_models[j] for j in range(k_folds) if j != i], X_val,
                                    weights=[weights[j] for j in range(k_folds) if j != i])
    y_pred = target_scaler.inverse_transform(y_pred_scaled)
    y_true = target_scaler.inverse_transform(y_val.reshape(-1, 1))
    all_preds.append(y_pred)
    all_actuals.append(y_true)

all_preds = np.concatenate(all_preds)
all_actuals = np.concatenate(all_actuals)
ensemble_mse = mean_squared_error(all_actuals, all_preds)
ensemble_rmse = np.sqrt(ensemble_mse)
ensemble_mae = mean_absolute_error(all_actuals, all_preds)
ensemble_r2 = r2_score(all_actuals, all_preds)
print(f"Ensemble RMSE: {ensemble_rmse:.2f}")
print(f"Ensemble MAE: {ensemble_mae:.2f}")
print(f"Ensemble R²: {ensemble_r2:.4f}")
\end{lstlisting}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Données} & \textbf{RMSE} & \textbf{MAE} & \textbf{R\textsuperscript{2}} \\
\hline
Entraînement (moyen) & 48\,000.00 & 29\,000.00 & 0.900 \\
\hline
Validation (ensemble) & 54\,500.00 & 33\,500.00 & 0.795 \\
\hline
\end{tabular}
\caption{Performances du modèle RNA sur les jeux d’entraînement et de validation}
\label{tab:nn_performance}
\end{table}

Le RMSE moyen de 54\,500 sur la validation indique une erreur acceptable pour des prix de véhicules (souvent entre 10\,000 et 100\,000), et le R² de 0,795 montre que le modèle explique 79,5~\% de la variance, une performance solide pour une tâche de régression complexe grâce à l’ensemble.


\subsection{Évaluation comparative des modèles}




% =======================
% Chapter 5: Résultat final
% =======================
\chapter{Résultat final}

\section{Prédiction des prix : Tests et observations}

Dans cette section, nous présentons les résultats des prédictions de prix effectuées sur des annonces réelles extraites des plateformes \texttt{avito.ma} et \texttt{moteur.ma}, non incluses dans les données d’entraînement. Ces tests visent à évaluer la performance des modèles développés (Random Forest, LightGBM, XGBoost, Réseau de neurones artificiels) dans des scénarios réels, en analysant l’impact de modifications des caractéristiques (kilométrage, origine, etc.) sur les prix prédits. Nous abordons également les limites observées, notamment pour les voitures de luxe.

\subsection{Test 1 : Analyse d’une annonce standard}

Nous avons sélectionné une annonce représentative d’une voiture populaire sur \texttt{avito.ma}, avec les caractéristiques suivantes : marque, modèle, année, kilométrage, type de carburant, transmission, et équipements. L’objectif était de prédire le prix et d’évaluer la sensibilité du modèle à des variations dans les caractéristiques.

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/test_1.png}
    \caption{Annonce initiale sélectionnée pour le test}
    \label{fig:test_1}
\end{figure}

Le modèle a prédit un prix cohérent avec les données du marché, comme illustré ci-dessous :

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/test_1-1.png}
    \caption{Prédiction initiale pour l’annonce}
    \label{fig:result_1}
\end{figure}

Pour tester la robustesse du modèle, nous avons modifié le kilométrage de la voiture, le passant à 200\,000 km. Le prix prédit a diminué d’environ 2\,000 Dhs, ce qui reflète l’impact attendu d’un kilométrage plus élevé sur la valeur du véhicule.

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/result_1-2.png}
    \caption{Prédiction avec kilométrage modifié (200\,000 km)}
    \label{fig:result_1_2}
\end{figure}

Nous avons également modifié l’origine du véhicule (par exemple, de « Dédouanée » à « Importée »). Cette modification a entraîné une variation du prix prédit, démontrant la capacité du modèle à capturer l’influence des caractéristiques catégoriques.

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/result_1-3.png}
    \caption{Prédiction avec origine modifiée}
    \label{fig:result_1_3}
\end{figure}

Ces résultats confirment que le modèle est sensible aux variations des caractéristiques clés et produit des prédictions alignées avec les tendances du marché pour les voitures populaires.

\subsection{Test 2 : Prédiction pour une annonce sans prix}

Certaines annonces sur \texttt{avito.ma} et \texttt{moteur.ma} ne précisent pas de prix, ce qui représente un cas d’usage idéal pour notre système de prédiction. Nous avons sélectionné une annonce sans prix pour évaluer la capacité du modèle à fournir une estimation fiable.

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/test_2.png}
    \caption{Annonce sans prix pour le test}
    \label{fig:test_2}
\end{figure}

Le modèle a généré une prédiction de prix cohérente, comme illustré ci-dessous :

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/test_2-1.png}
    \caption{Prédiction initiale pour l’annonce sans prix}
    \label{fig:test_2_1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/test_2-2.png}
    \caption{Prédiction après modification du kilométrage}
    \label{fig:test_2_2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/test_2-3.png}
    \caption{Prédiction après modification du type de carburant}
    \label{fig:test_2_3}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/test_2-4.png}
    \caption{Prédiction après modification des équipements}
    \label{fig:test_2_4}
\end{figure}

Ces tests démontrent que notre système peut être utilisé comme un service de prédiction de prix pour les annonces sans prix, offrant une estimation fiable aux utilisateurs.

\subsection{Limitation : Prédictions pour les voitures de luxe}

Une limitation notable a été observée lors de la prédiction des prix pour les voitures de luxe, comme illustré par une annonce de Land Rover.

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/test_2-5.png}
    \caption{Annonce d’une voiture de luxe (Land Rover)}
    \label{fig:test_2_5}
\end{figure}

La prédiction pour cette annonce s’est révélée moins précise, comme montré ci-dessous :

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/test_2-6.png}
    \caption{Prédiction pour la voiture de luxe}
    \label{fig:test_2_6}
\end{figure}

Pour comprendre cette limitation, nous avons analysé la distribution des marques dans notre jeu de données. En examinant les données nettoyées (\texttt{cleaned_data.csv}), nous avons constaté une sous-représentation des voitures de luxe :

\begin{lstlisting}[style=pythonstyle]
import pandas as pd

# Load the CSV file
df = pd.read_csv('cleaned_data.csv')

# Count the number of rows where the brand is 'Land Rover'
land_rover_count = df[df['brand'].str.lower() == 'land rover'].shape[0]

# Output the result
print(f"The number of 'Land Rover' entries in the dataset is: {land_rover_count}")
\end{lstlisting}

\begin{tcolorbox}
\begin{verbatim}
The number of 'Land Rover' entries in the dataset is: 500
\end{verbatim}
\end{tcolorbox}

Sur un total de 60\,000 annonces, seules 500 concernent des Land Rover, soit environ 0,9\,\% du jeu de données. Cette sous-représentation est typique des plateformes comme \texttt{avito.ma} et \texttt{moteur.ma}, où les voitures populaires (Volkswagen, Renault, Peugeot) dominent, et les véhicules de luxe ou haut de gamme sont moins fréquents. Par conséquent, les modèles n’ont pas été suffisamment entraînés sur ces catégories, ce qui entraîne des prédictions moins précises pour les voitures de luxe.

\subsection{Discussion et implications}

Les tests montrent que nos modèles performent bien pour les voitures populaires, avec des prédictions sensibles aux variations des caractéristiques clés (kilométrage, origine, équipements). Le système est particulièrement utile pour estimer les prix des annonces sans prix, offrant une valeur ajoutée aux utilisateurs des plateformes automobiles marocaines. Cependant, la faible représentation des voitures de luxe dans les données limite la précision des prédictions pour ce segment.

Pour améliorer les performances sur les voitures de luxe, nous recommandons :
\begin{itemize}
    \item \textbf{Augmentation des données} : Collecter davantage d’annonces de voitures haut de gamme via des sources spécialisées ou des techniques d’augmentation de données.
    \item \textbf{Modèles spécifiques} : Entraîner un modèle dédié aux voitures de luxe, en utilisant un sous-ensemble de données enrichi pour ce segment.
    \item \textbf{Pondération des classes} : Ajuster les modèles pour accorder plus de poids aux observations rares lors de l’entraînement.
\end{itemize}

Ces améliorations permettraient d’élargir la portée du système et de renforcer sa fiabilité pour tous les segments du marché automobile marocain.
\section{Recommandations : Tests et observations}

% Introducing the user scenario for testing the recommendation system
Dans cette section, nous évaluons le système de recommandation à travers le scénario d’un utilisateur fictif, Hamza Bjibji, afin de démontrer comment les algorithmes (filtrage collaboratif basé sur les utilisateurs, basé sur les objets, basé sur le contenu, et hybride) génèrent des recommandations personnalisées en fonction des préférences et interactions enregistrées.

\subsection{Profil de l’utilisateur}

Hamza Bjibji, âgé de 22 ans, réside à Tétouan. Lors de son inscription sur la plateforme, il renseigne ses préférences initiales via la page d’inscription :

\begin{itemize}
    \item \textbf{Marques préférées} : Volkswagen, Renault
    \item \textbf{Type de carburant} : Diesel
    \item \textbf{Transmission} : Automatique ou manuelle
    \item \textbf{Budget} : Entre 10\,000 et 20\,000 Dhs
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/test_3.png}
    \caption{Page d’inscription de Hamza Bjibji avec ses préférences initiales}
    \label{fig:test_3}
\end{figure}

Ces informations sont enregistrées dans la table \texttt{user_preferences} de la base de données, comme illustré ci-dessous :

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/test_3-6.png}
    \caption{Tableau des préférences utilisateur dans la base de données}
    \label{fig:test_3_6}
\end{figure}

\subsection{Modification des préférences}

Après son inscription, Hamza se connecte à la plateforme et accède à la section de gestion de son profil pour ajuster ses préférences. Par exemple, il peut préciser un modèle spécifique (Golf ou Dacia) ou modifier son budget.

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/test_3-2.png}
    \caption{Interface de modification des préférences de Hamza}
    \label{fig:test_3_2}
\end{figure}

Ces modifications sont immédiatement mises à jour dans la base de données, permettant au système de recommandation de s’adapter aux nouvelles préférences.

\subsection{Interactions avec la plateforme}

Hamza navigue sur la plateforme et effectue diverses actions enregistrées dans la base de données, notamment :

\begin{itemize}
    \item \textbf{Recherches} : Hamza effectue des recherches sur des modèles spécifiques, comme Volkswagen Golf et Dacia.
    \item \textbf{Vues} : Il consulte plusieurs annonces, et le temps passé sur chaque annonce (en secondes) ainsi que la source de navigation (mobile, web) sont enregistrés.
    \item \textbf{J’aime} : Hamza marque certaines annonces comme « J’aime », indiquant un intérêt particulier.
    \item \textbf{Sauvegardes} : Il sauvegarde des annonces pour consultation ultérieure.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/test_3-3.png}
    \caption{Interface de navigation de Hamza sur la plateforme}
    \label{fig:test_3_3}
\end{figure}

Les recherches effectuées par Hamza sont enregistrées dans la base de données, comme illustré ci-dessous :

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/test_3-5.png}
    \caption{Enregistrement des recherches de Hamza (Golf et Dacia)}
    \label{fig:test_3_5}
\end{figure}

Les annonces marquées comme « J’aime » sont également stockées :

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/test_3-7.png}
    \caption{Tableau des annonces marquées comme « J’aime » par Hamza}
    \label{fig:test_3_7}
\end{figure}

Les vues, incluant le temps passé et la source de navigation, sont enregistrées pour analyser le comportement de Hamza :

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/test_3-8.png}
    \caption{Tableau des vues enregistrées pour Hamza}
    \label{fig:test_3_8}
\end{figure}

\subsection{Génération des recommandations}

Une fois les données collectées, le système de recommandation est lancé en temps réel pour générer des suggestions personnalisées pour Hamza. Les algorithmes exploitent :

\begin{itemize}
    \item \textbf{Préférences explicites} : Marque (Volkswagen, Renault), carburant (diesel), transmission (automatique ou manuelle), budget (10\,000–20\,000 Dhs).
    \item \textbf{Interactions implicites} : Recherches (Golf, Dacia), vues, « J’aime », et annonces sauvegardées.
\end{itemize}

Le modèle hybride, combinant filtrage collaboratif et basé sur le contenu, est utilisé pour produire les recommandations. Les résultats sont présentés à Hamza via une interface dédiée :

\textbf{Lancement de la recommandation}

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/test_3-10.png}
    \caption{Recommandations personnalisées générées pour Hamza}
    \label{fig:test_3_10}
\end{figure}

\textbf{Les recommandations :} Il prend environ 2 secondes pour charger les recommandations, entre l'exécution des algorithmes et la récupération des données via l'API.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/test_3-12.png}
    \caption{Affichage des recommandations personnalisées pour Hamza sur l'interface utilisateur}
    \label{fig:test_3_12}
\end{figure}

\textbf{Extrait de la base de données :} Les recommandations générées sont stockées dans la table user_recommendations. Le tableau \ref{tab:recommendations_hamza} présente les recommandations pour Hamza, incluant l’identifiant de la voiture, la date de création, la méthode utilisée, la raison de la recommandation et le score de similarité.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/test_3-13.png}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}



Les recommandations incluent des annonces correspondant aux critères de Hamza, comme des Volkswagen ou Renault diesel dans son budget, avec des explications interprétables (par exemple, « Correspond à votre préférence pour une transmission manuelle »). Le système priorise également les annonces similaires à celles qu’il a marquées comme « J’aime » ou consultées longuement.

\subsection{Évaluation des recommandations}

Pour évaluer la pertinence des recommandations, nous avons mesuré la précision@5 (proportion des 5 premières recommandations jugées pertinentes) sur un ensemble de profils utilisateurs simulés, incluant celui de Hamza. Le modèle hybride a atteint une précision@5 de 0,20, comme indiqué dans le tableau \ref{tab:recommendation_performance} du chapitre précédent, surpassant les approches basées uniquement sur le contenu (0,18) ou le filtrage collaboratif (0,15).

Dans le cas de Hamza, les recommandations ont été jugées pertinentes, car elles correspondaient à ses préférences explicites (marque, budget) et reflétaient ses interactions (recherches sur Golf et Dacia). Cependant, pour améliorer la précision, il serait bénéfique de collecter davantage d’interactions au fil du temps, permettant au système d’affiner les profils utilisateurs.

\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{images/test_3-1.png}
    \caption{Résultats des recommandations affichées à Hamza}
    \label{fig:test_3_1}
\end{figure}

\subsection{Discussion}

Ce scénario démontre que le système de recommandation est capable de générer des suggestions personnalisées en temps réel, en s’appuyant sur les préférences et interactions des utilisateurs. L’enregistrement détaillé des actions (recherches, vues, « J’aime ») dans la base de données permet une analyse fine du comportement, renforçant la pertinence des recommandations. Cependant, des tests avec des utilisateurs réels et une collecte continue de données sont nécessaires pour optimiser les algorithmes, notamment pour les utilisateurs ayant peu d’interactions initiales.

\end{document} je sais pâs pourqou la dernire chapitre est avec font different que les autres font du rapport